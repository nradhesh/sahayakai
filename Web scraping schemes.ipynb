{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d05f5a-f9d3-45a1-af4c-0d3a5f575b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages or error navigating to the next page. Stopping.\n",
      "Scraped 2439 links.\n"
     ]
    }
   ],
   "source": [
    "#click on the right arrow\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# Function to extract links from the current page\n",
    "def extract_links_from_page():\n",
    "    links = []\n",
    "    divs = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.p-4.lg\\\\:p-8.w-full\")))\n",
    "    for div in divs:\n",
    "        try:\n",
    "            a_tag = div.find_element(By.XPATH, \".//h2/a\")\n",
    "            link = a_tag.get_attribute(\"href\")\n",
    "            if link:\n",
    "                links.append(link)\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding link in div: {e}\")\n",
    "    return links\n",
    "\n",
    "# Open the initial URL\n",
    "driver.get(\"https://www.myscheme.gov.in/search\")\n",
    "\n",
    "all_links = []\n",
    "\n",
    "# Loop through each page and extract links\n",
    "while True:\n",
    "    all_links.extend(extract_links_from_page())\n",
    "    try:\n",
    "        next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".ml-2.text-darkblue-900.dark\\\\:text-white.cursor-pointer\")))\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "        time.sleep(1)  # Wait for scrolling to complete\n",
    "        next_button.click()\n",
    "        time.sleep(2)  # Adjust the sleep time if needed to allow the next page to load properly\n",
    "    except Exception as e:\n",
    "        print(\"No more pages or error navigating to the next page. Stopping.\")\n",
    "        break\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "print(f\"Scraped {len(all_links)} links.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08366001-ff96-4b64-bfad-45330eeff9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b5c67203-216e-456d-af08-8bbeba1eeb3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_links' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m             duplicates\u001b[38;5;241m.\u001b[39mappend((item, indexes))\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m duplicates\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mprint\u001b[39m(find_duplicates_with_indexes(\u001b[43mall_links\u001b[49m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_links' is not defined"
     ]
    }
   ],
   "source": [
    "#find the duplicate links in the list\n",
    "\n",
    "def find_duplicates_with_indexes(lst):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    counts = defaultdict(list)\n",
    "    duplicates = []\n",
    "\n",
    "    for index, item in enumerate(lst):\n",
    "        counts[item].append(index)\n",
    "\n",
    "    for item, indexes in counts.items():\n",
    "        if len(indexes) > 1:\n",
    "            duplicates.append((item, indexes))\n",
    "\n",
    "    return duplicates\n",
    "\n",
    "\n",
    "print(find_duplicates_with_indexes(all_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "819fc243-f31f-4504-86de-30e61ffe960f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n"
     ]
    }
   ],
   "source": [
    "#Remove duplicates\n",
    "\n",
    "def remove_duplicates(lst):\n",
    "    seen = set()\n",
    "    unique_links = []\n",
    "    for item in lst:\n",
    "        if item not in seen:\n",
    "            unique_links.append(item)\n",
    "            seen.add(item)\n",
    "    return unique_links\n",
    "\n",
    "\n",
    "unique_links = remove_duplicates(all_links_headless)\n",
    "print(len(unique_links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0178a5d1-9b54-4448-859e-02f279e84e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m unique_links:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m url \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m existing_links:\n\u001b[0;32m---> 52\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_scheme_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m sample:\n\u001b[1;32m     54\u001b[0m             new_data\u001b[38;5;241m.\u001b[39mappend(sample)\n",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m, in \u001b[0;36mscrape_scheme_page\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     11\u001b[0m detail \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetails\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mif\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetails\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m benefit \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbenefits\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mif\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbenefits\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 13\u001b[0m eligibility \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meligibility\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meligibility\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m exclusion \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexclusions\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mif\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexclusions\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m application_process \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication-process\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mif\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication-process\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/bs4/element.py:2006\u001b[0m, in \u001b[0;36mTag.find\u001b[0;34m(self, name, attrs, recursive, string, **kwargs)\u001b[0m\n\u001b[1;32m   1989\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Look in the children of this PageElement and find the first\u001b[39;00m\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;124;03mPageElement that matches the given criteria.\u001b[39;00m\n\u001b[1;32m   1991\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;124;03m:rtype: bs4.element.Tag | bs4.element.NavigableString\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2006\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2007\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2008\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m l:\n\u001b[1;32m   2009\u001b[0m     r \u001b[38;5;241m=\u001b[39m l[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/bs4/element.py:2035\u001b[0m, in \u001b[0;36mTag.find_all\u001b[0;34m(self, name, attrs, recursive, string, limit, **kwargs)\u001b[0m\n\u001b[1;32m   2033\u001b[0m     generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren\n\u001b[1;32m   2034\u001b[0m _stacklevel \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_stacklevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m-> 2035\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2036\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/bs4/element.py:841\u001b[0m, in \u001b[0;36mPageElement._find_all\u001b[0;34m(self, name, attrs, string, limit, generator, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i:\n\u001b[0;32m--> 841\u001b[0m     found \u001b[38;5;241m=\u001b[39m \u001b[43mstrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m found:\n\u001b[1;32m    843\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(found)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/bs4/element.py:2325\u001b[0m, in \u001b[0;36mSoupStrainer.search\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(markup, Tag):\n\u001b[1;32m   2324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstring \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs:\n\u001b[0;32m-> 2325\u001b[0m         found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[38;5;66;03m# If it's text, make sure the text matches.\u001b[39;00m\n\u001b[1;32m   2327\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(markup, NavigableString) \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[1;32m   2328\u001b[0m          \u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/bs4/element.py:2288\u001b[0m, in \u001b[0;36mSoupStrainer.search_tag\u001b[0;34m(self, markup_name, markup_attrs)\u001b[0m\n\u001b[1;32m   2286\u001b[0m             markup_attr_map[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[1;32m   2287\u001b[0m attr_value \u001b[38;5;241m=\u001b[39m markup_attr_map\u001b[38;5;241m.\u001b[39mget(attr)\n\u001b[0;32m-> 2288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_matches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch_against\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2289\u001b[0m     match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2290\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/bs4/element.py:2356\u001b[0m, in \u001b[0;36mSoupStrainer._matches\u001b[0;34m(self, markup, match_against, already_tried)\u001b[0m\n\u001b[1;32m   2352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m match_against \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   2353\u001b[0m     \u001b[38;5;66;03m# True matches any non-None value.\u001b[39;00m\n\u001b[1;32m   2354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m markup \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmatch_against\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCallable\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m match_against(markup)\n\u001b[1;32m   2359\u001b[0m \u001b[38;5;66;03m# Custom callables take the tag as an argument, but all\u001b[39;00m\n\u001b[1;32m   2360\u001b[0m \u001b[38;5;66;03m# other ways of matching match the tag name as a string.\u001b[39;00m\n",
      "File \u001b[0;32m<frozen abc>:119\u001b[0m, in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to scrape data from each scheme page\n",
    "def scrape_scheme_page(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            return None\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        scheme_name = soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\").text.strip() if soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\") else 'N/A'\n",
    "        detail = soup.find('div', id='details').text.strip() if soup.find('div', id='details') else 'N/A'\n",
    "        benefit = soup.find('div', id='benefits').text.strip() if soup.find('div', id='benefits') else 'N/A'\n",
    "        eligibility = soup.find('div', id='eligibility').text.strip() if soup.find('div', id='eligibility') else 'N/A'\n",
    "        exclusion = soup.find('div', id='exclusions').text.strip() if soup.find('div', id='exclusions') else 'N/A'\n",
    "        application_process = soup.find('div', id='application-process').text.strip() if soup.find('div', id='application-process') else 'N/A'\n",
    "        documents_required = soup.find('div', id='documents-required').text.strip() if soup.find('div', id='documents-required') else 'N/A'\n",
    "        faq = soup.find('div', id='faqs').text.strip() if soup.find('div', id='faqs') else 'N/A'\n",
    "        keyword = soup.find('div', class_='mb-2 md:mb-0 w-full').text.strip() if soup.find('div', class_='mb-2 md:mb-0 w-full') else 'N/A'\n",
    "        \n",
    "        return {\n",
    "            'SchemeName': scheme_name,\n",
    "            'Details': detail,\n",
    "            'Benefits': benefit,\n",
    "            'Eligibility': eligibility,\n",
    "            'Exclusion': exclusion,\n",
    "            'ApplicationProc': application_process,\n",
    "            'DocumentsReq': documents_required,\n",
    "            'FAQ': faq,\n",
    "            'Link': url,\n",
    "            'Keywords': keyword\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load existing data if available\n",
    "if os.path.exists('scheme_sample.csv'):\n",
    "    existing_data = pd.read_csv('scheme_sample.csv')\n",
    "    existing_links = set(existing_data['Link'])\n",
    "    print('A')\n",
    "else:\n",
    "    existing_data = pd.DataFrame()\n",
    "    existing_links = set()\n",
    "    print('B')\n",
    "\n",
    "# Initialize lists to store new data\n",
    "new_data = []\n",
    "\n",
    "# Scrape each URL and append data to lists\n",
    "for url in unique_links:\n",
    "    if url not in existing_links:\n",
    "        sample = scrape_scheme_page(url)\n",
    "        if sample:\n",
    "            new_data.append(sample)\n",
    "\n",
    "# Create a pandas DataFrame for new data\n",
    "new_data_df = pd.DataFrame(new_data)\n",
    "\n",
    "# Combine new data with existing data\n",
    "if not existing_data.empty:\n",
    "    combined_data = pd.concat([existing_data, new_data_df]).drop_duplicates(subset='Link', keep='last')\n",
    "    print('a')\n",
    "else:\n",
    "    combined_data = new_data_df\n",
    "    print('b')\n",
    "\n",
    "# Save the combined data to CSV file\n",
    "combined_data.to_csv('scheme_sample.csv', index=False)\n",
    "\n",
    "print(\"Scraping and updating complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d74015d1-4158-4c6b-aa88-371732202626",
   "metadata": {},
   "outputs": [],
   "source": [
    "LINKS = unique_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7bbab51a-04e7-4ceb-b79b-d725249c5572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3331547-5600-440b-b790-4dd3f401e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_links.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "24f3fcc6-2dea-4585-b646-766a1f158d9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping and updating complete.\n"
     ]
    }
   ],
   "source": [
    "#84 seconds but gives error\n",
    "#157 seconds but no error\n",
    "#143 seconds and no error\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# Function to scrape data from each scheme page\n",
    "def scrape_scheme_page(url):\n",
    "    retries = 3\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch {url} with status code {response.status_code}\")\n",
    "                return None\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            scheme_name = soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\").text.strip() if soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\") else 'N/A'\n",
    "            detail = soup.find('div', id='details').text.strip() if soup.find('div', id='details') else 'N/A'\n",
    "            benefit = soup.find('div', id='benefits').text.strip() if soup.find('div', id='benefits') else 'N/A'\n",
    "            eligibility = soup.find('div', id='eligibility').text.strip() if soup.find('div', id='eligibility') else 'N/A'\n",
    "            exclusion = soup.find('div', id='exclusions').text.strip() if soup.find('div', id='exclusions') else 'N/A'\n",
    "            application_process = soup.find('div', id='application-process').text.strip() if soup.find('div', id='application-process') else 'N/A'\n",
    "            documents_required = soup.find('div', id='documents-required').text.strip() if soup.find('div', id='documents-required') else 'N/A'\n",
    "            faq = soup.find('div', id='faqs').text.strip() if soup.find('div', id='faqs') else 'N/A'\n",
    "            keyword = soup.find('div', class_='mb-2 md:mb-0 w-full').text.strip() if soup.find('div', class_='mb-2 md:mb-0 w-full') else 'N/A'\n",
    "            \n",
    "            return {\n",
    "                'SchemeName': scheme_name,\n",
    "                'Details': detail,\n",
    "                'Benefits': benefit,\n",
    "                'Eligibility': eligibility,\n",
    "                'Exclusion': exclusion,\n",
    "                'ApplicationProc': application_process,\n",
    "                'DocumentsReq': documents_required,\n",
    "                'FAQ': faq,\n",
    "                'Link': url,\n",
    "                'Keywords': keyword\n",
    "            }\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request error scraping {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "        time.sleep(1)  # Sleep before retrying\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    # Load existing data if available\n",
    "    if os.path.exists('scheme_sample.csv'):\n",
    "        existing_data = pd.read_csv('scheme_sample.csv')\n",
    "        existing_links = set(existing_data['Link'])\n",
    "    else:\n",
    "        existing_data = pd.DataFrame()\n",
    "        existing_links = set()\n",
    "\n",
    "    # Initialize lists to store new data\n",
    "    new_data = []\n",
    "\n",
    "    # Use ThreadPoolExecutor to manage concurrent scraping\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(scrape_scheme_page, url) for url in unique_links if url not in existing_links]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                new_data.append(result)\n",
    "\n",
    "    # Create a pandas DataFrame for new data\n",
    "    new_data_df = pd.DataFrame(new_data)\n",
    "\n",
    "    # Combine new data with existing data\n",
    "    if not existing_data.empty:\n",
    "        combined_data = pd.concat([existing_data, new_data_df]).drop_duplicates(subset='Link', keep='last')\n",
    "    else:\n",
    "        combined_data = new_data_df\n",
    "\n",
    "    # Save the combined data to CSV file\n",
    "    combined_data.to_csv('scheme_sample.csv', index=False)\n",
    "\n",
    "    print(\"Scraping and updating complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "055a6aca-7ec7-429c-bb22-53f6da0ea07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping and updating complete.\n"
     ]
    }
   ],
   "source": [
    "#64 seconds and no error\n",
    "#50 seconds and no error\n",
    "#53 seconds and no error\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# Function to create a requests session with retries\n",
    "def requests_session_with_retries(retries=3, backoff_factor=1):\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=(500, 502, 503, 504),\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "# Function to scrape data from each scheme page\n",
    "def scrape_scheme_page(session, url):\n",
    "    try:\n",
    "        response = session.get(url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {url} with status code {response.status_code}\")\n",
    "            return None\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        scheme_name = soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\").text.strip() if soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\") else 'N/A'\n",
    "        detail = soup.find('div', id='details').text.strip() if soup.find('div', id='details') else 'N/A'\n",
    "        benefit = soup.find('div', id='benefits').text.strip() if soup.find('div', id='benefits') else 'N/A'\n",
    "        eligibility = soup.find('div', id='eligibility').text.strip() if soup.find('div', id='eligibility') else 'N/A'\n",
    "        exclusion = soup.find('div', id='exclusions').text.strip() if soup.find('div', id='exclusions') else 'N/A'\n",
    "        application_process = soup.find('div', id='application-process').text.strip() if soup.find('div', id='application-process') else 'N/A'\n",
    "        documents_required = soup.find('div', id='documents-required').text.strip() if soup.find('div', id='documents-required') else 'N/A'\n",
    "        faq = soup.find('div', id='faqs').text.strip() if soup.find('div', id='faqs') else 'N/A'\n",
    "        keyword = soup.find('div', class_='mb-2 md:mb-0 w-full').text.strip() if soup.find('div', class_='mb-2 md:mb-0 w-full') else 'N/A'\n",
    "        \n",
    "        return {\n",
    "            'SchemeName': scheme_name,\n",
    "            'Details': detail,\n",
    "            'Benefits': benefit,\n",
    "            'Eligibility': eligibility,\n",
    "            'Exclusion': exclusion,\n",
    "            'ApplicationProc': application_process,\n",
    "            'DocumentsReq': documents_required,\n",
    "            'FAQ': faq,\n",
    "            'Link': url,\n",
    "            'Keywords': keyword\n",
    "        }\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request error scraping {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    # Load existing data if available\n",
    "    if os.path.exists('scheme_sample2.csv'):\n",
    "        existing_data = pd.read_csv('scheme_sample2.csv')\n",
    "        existing_links = set(existing_data['Link'])\n",
    "    else:\n",
    "        existing_data = pd.DataFrame()\n",
    "        existing_links = set()\n",
    "\n",
    "    # Initialize lists to store new data\n",
    "    new_data = []\n",
    "\n",
    "    # Create a requests session with retries\n",
    "    session = requests_session_with_retries()\n",
    "\n",
    "    # Use ThreadPoolExecutor to manage concurrent scraping\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(scrape_scheme_page, session, url) for url in unique_links if url not in existing_links]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                new_data.append(result)\n",
    "\n",
    "    # Create a pandas DataFrame for new data\n",
    "    new_data_df = pd.DataFrame(new_data)\n",
    "\n",
    "    # Combine new data with existing data\n",
    "    if not existing_data.empty:\n",
    "        combined_data = pd.concat([existing_data, new_data_df]).drop_duplicates(subset='Link', keep='last')\n",
    "    else:\n",
    "        combined_data = new_data_df\n",
    "\n",
    "    # Save the combined data to CSV file\n",
    "    combined_data.to_csv('scheme_sample2.csv', index=False)\n",
    "\n",
    "    print(\"Scraping and updating complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e1cec38-8fc5-4163-b8eb-cfd9fd9cec7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping and updating complete.\n"
     ]
    }
   ],
   "source": [
    "#trial with less data\n",
    "#64 seconds and no error\n",
    "#50 seconds and no error\n",
    "#53 seconds and no error\n",
    "#for 50 extra data, 3 seconds\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# Function to create a requests session with retries\n",
    "def requests_session_with_retries(retries=3, backoff_factor=1):\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=(500, 502, 503, 504),\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "# Function to scrape data from each scheme page\n",
    "def scrape_scheme_page(session, url):\n",
    "    try:\n",
    "        response = session.get(url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {url} with status code {response.status_code}\")\n",
    "            return None\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        scheme_name = soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\").text.strip() if soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\") else 'N/A'\n",
    "        detail = soup.find('div', id='details').text.strip() if soup.find('div', id='details') else 'N/A'\n",
    "        benefit = soup.find('div', id='benefits').text.strip() if soup.find('div', id='benefits') else 'N/A'\n",
    "        eligibility = soup.find('div', id='eligibility').text.strip() if soup.find('div', id='eligibility') else 'N/A'\n",
    "        exclusion = soup.find('div', id='exclusions').text.strip() if soup.find('div', id='exclusions') else 'N/A'\n",
    "        application_process = soup.find('div', id='application-process').text.strip() if soup.find('div', id='application-process') else 'N/A'\n",
    "        documents_required = soup.find('div', id='documents-required').text.strip() if soup.find('div', id='documents-required') else 'N/A'\n",
    "        faq = soup.find('div', id='faqs').text.strip() if soup.find('div', id='faqs') else 'N/A'\n",
    "        keyword = soup.find('div', class_='mb-2 md:mb-0 w-full').text.strip() if soup.find('div', class_='mb-2 md:mb-0 w-full') else 'N/A'\n",
    "        \n",
    "        return {\n",
    "            'SchemeName': scheme_name,\n",
    "            'Details': detail,\n",
    "            'Benefits': benefit,\n",
    "            'Eligibility': eligibility,\n",
    "            'Exclusion': exclusion,\n",
    "            'ApplicationProc': application_process,\n",
    "            'DocumentsReq': documents_required,\n",
    "            'FAQ': faq,\n",
    "            'Link': url,\n",
    "            'Keywords': keyword\n",
    "        }\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request error scraping {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    # Load existing data if available\n",
    "    if os.path.exists('scheme_sample3.csv'):\n",
    "        existing_data = pd.read_csv('scheme_sample3.csv')\n",
    "        existing_links = set(existing_data['Link'])\n",
    "    else:\n",
    "        existing_data = pd.DataFrame()\n",
    "        existing_links = set()\n",
    "\n",
    "    # Initialize lists to store new data\n",
    "    new_data = []\n",
    "\n",
    "    # Create a requests session with retries\n",
    "    session = requests_session_with_retries()\n",
    "\n",
    "    # Use ThreadPoolExecutor to manage concurrent scraping\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(scrape_scheme_page, session, url) for url in unique_links if url not in existing_links]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                new_data.append(result)\n",
    "\n",
    "    # Create a pandas DataFrame for new data\n",
    "    new_data_df = pd.DataFrame(new_data)\n",
    "\n",
    "    # Combine new data with existing data\n",
    "    if not existing_data.empty:\n",
    "        combined_data = pd.concat([existing_data, new_data_df]).drop_duplicates(subset='Link', keep='last')\n",
    "    else:\n",
    "        combined_data = new_data_df\n",
    "\n",
    "    # Save the combined data to CSV file\n",
    "    combined_data.to_csv('scheme_sample3.csv', index=False)\n",
    "\n",
    "    print(\"Scraping and updating complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc2c1227-01a9-4b2a-a587-688962afe4a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sorted_scheme_sample2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m file1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msorted_scheme_sample.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     17\u001b[0m file2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msorted_scheme_sample2.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcompare_csv_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile2\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe contents of the two CSV files are the same.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[45], line 6\u001b[0m, in \u001b[0;36mcompare_csv_files\u001b[0;34m(file1, file2)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompare_csv_files\u001b[39m(file1, file2):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Read the CSV files\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     df1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file1)\n\u001b[0;32m----> 6\u001b[0m     df2 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Check if the shapes of the dataframes are the same\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df1\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m df2\u001b[38;5;241m.\u001b[39mshape:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sorted_scheme_sample2.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compare_csv_files(file1, file2):\n",
    "    # Read the CSV files\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "\n",
    "    # Check if the shapes of the dataframes are the same\n",
    "    if df1.shape != df2.shape:\n",
    "        return False\n",
    "\n",
    "    # Check if the dataframes are equal\n",
    "    return df1.equals(df2)\n",
    "\n",
    "# Example usage\n",
    "file1 = 'sorted_scheme_sample.csv'\n",
    "file2 = 'sorted_scheme_sample2.csv'\n",
    "\n",
    "if compare_csv_files(file1, file2):\n",
    "    print(\"The contents of the two CSV files are the same.\")\n",
    "else:\n",
    "    print(\"The contents of the two CSV files are different.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a4210ef-6e8a-49b5-8366-f38da49a2a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files differ at 20505 positions.\n",
      "Showing the first few differences:\n",
      "Row 0, Column 'SchemeName': File1='40% Subsidy On Hank Yarn, Dyes & Chemicals Scheme' vs File2='Aponar Apon Ghar'\n",
      "Row 0, Column 'Details': File1='DetailsThe \"40% subsidy on Hank Yarn, Dyes & Chemicals Scheme\" is a Subsidy Scheme by the Department of Handlooms & Textiles, Government of Andhra Pradesh. The entire assistance under the scheme will be in the form of Grant from the State Government. The scheme is operative from 29th April 2011. The subsidy will be available only on purchases / procurements from NHDC & APCO. The amount sanctioned will be credited directly to the members bank account of the concerned primary weavers cooperative societies.' vs File2='Details\"Aponar Apon Ghar\" is a Home Loan Subsidy Scheme by the Finance Department, Govt. of Assam. The objective is to realize the vision of Housing For All by 2022 so that each poor person may have their own house. All the applicants will get home loans at subsidized interest rates. Applicant must avail of housing loans from any Scheduled Commercial Bank, Regional Rural Bank, or Assam Cooperative Apex bank within the state. The state govt. will provide  2,50,000 subsidy on home loans up to  40,00,000. This house loan subsidy is only for those who are purchasing their 1st house and have not availed of loans under the previous Apun Ghar Scheme. The scope of the scheme is limited to the permanent residents of the state of Assam.'\n",
      "Row 0, Column 'Benefits': File1='Benefits40% subsidy on the purchases / procurements of Hank Yarn, Dyes & Chemicals from NHDC & APCO.The 75% amount will be credited directly to the members bank account of the concerned primary weavers cooperative societies as production bonus basing on the wages earned by them.The remaining 25% amount may be utilized by the societies for giving rebate on sales or any other production related purpose.' vs File2='BenefitsLoan Amount (): 5,00,000 - 10,00,000Subsidy (): 1,00,000Loan Amount (): 10,00,000 - 20,00,000Subsidy (): 1,50,000Loan Amount (): 20,00,000 - 30,00,000Subsidy (): 2,00,000Loan Amount (): 30,00,000 - 40,00,000Subsidy (): 2,50,000'\n",
      "Row 0, Column 'Eligibility': File1='EligibilityThe subsidy will be available only on purchase / procurement of yarn from NHDC, Yarn Deports sanctioned by NHDC and APCO for self-consumption of Handloom Weaver Cooperative Societies for providing work to weaver members.The subsidy will be allowed only on the Hank yarn purchased from the APCO & NHDC and its depots and utilized for the production on Societies, account during the quarter.The claims for subsidy shall be submitted by the Weavers Cooperative Societies in the prescribed proforma on quarterly basis during the financial year with in 15 days from the end of the respective quarter.The claims for subsidy in complete shape should reach Head Office with in 30 days from the end of the Quarter.NOTE 1: The Weavers Coop Societies shall NOT claim any subsidy on the Yarn, Dyes and Chemicals purchased from private yarn dealers, Mills etc.NOTE 2: The Weavers Coop. Societies should NOT claim any subsidy on Yarn, Dyes & Chemicals even if purchased from APCO or NHDC, but not utilized for production of cloth by way of issue of yarn and sold to non-members. ' vs File2='EligibilityThe applicants must be permanent residents of Assam state.The applicants must avail the housing loan from any Scheduled Commercial Bank, Regional Rural Banks, Assam Cooperative Apex bank within the state.The total family income of the applicant (from all sources) must not exceed  20,00,000.The housing loan must be of more than  5,00,000 and sanctioned by bank on or after 1st April 2019.The loan accounts must not be under NPA (Non Performing Assets) status.This must be the first home by the composite family.'\n",
      "Row 0, Column 'Exclusion': File1='ExclusionsThe Weavers Cooperative Societies shall not claim any subsidy on the Yarn, Dyes and Chemicals purchased from private yarn dealers, Mills etc.The Weavers Coop. Societies should not claim any subsidy on Yarn, Dyes & Chemicals even if purchased from APCO or NHDC, but not utilized for production of cloth by way of issue of yarn and sold to non-members.' vs File2='ExclusionsThose who already benefited under the Apon Ghar scheme are not eligible.'\n",
      "Row 0, Column 'ApplicationProc': File1='Application ProcessOfflineApplication:Step 1: The claims for subsidy shall be submitted by the Weavers Cooperative Societies to the Assistant Director (H&T) in the prescribed proforma on quarterly basis during the financial year within 15 days from the end of the respective quarter.Step 2: The AD(H&T) after scrutiny shall recommend the claims with in next 15 days.Step 3: All claims in complete shape should reach Head Office with in 30 days from the end of the Quarter.NOTE: Appropriate cut will be imposed on bleated claims.Disbursal of the Subsidy:The amount sanctioned as 40% Yarn subsidy, 75% amount will be credited directly to the members bank account of the concerned primary Weavers Cooperative Societies as production bonus basing on the wages earned by them.The societies may utilize the remaining 25% amount for giving rebate on sales or any other production related purpose with effect from 1st Dec 2016.' vs File2='Application ProcessOnlineStep 1: Visit the official website. Navigate to the \"Click here for Loan Subsidy\" section.Step 2: Click \"Aponar Apon Ghar (Home Loan Subsidy Scheme)\". You will be taken to \"Application for Release for Subsidy\". Fill in all the mandatory details.Applicant's Details: Name, Gender, DOB, PAN No., Email ID, Mobile Number, Address.Loan Details: IFSC of Loan Issuing Branch, Bank Name, Branch Name, Account Number, Loan Sanction Amount, Date of Loan Sanction, Property Address.Step 3: Upload the Supporting Documents: Land Proof, Address Proof, and PAN Card Proof. Check the Declaration, and click \"Save\".Check Application Status:Step 1: Visit the official website. At the bottom right of the page, in the \"Application Tracking\" section, click \"Track\".Step 2: On the next page, provide your Mobile Number, and Loan Account Number or Application Number, and click \"Submit\".'\n",
      "Row 0, Column 'DocumentsReq': File1='Documents RequiredCertification that subsidy has been claimed only on the Hank Yarn and Dyes & Chemicals supplied to the members and utilized for production on Societies account from out of the Yarn and Dyes & Chemicals purchased from the NHDC and APCO.Photocopies of invoices/receipts of hank yarn and Dyes & Chemicals purchased/procured from NHDC, its depots and APCO.The abstract duly attested by the concerned Assistant Director (H&T).Registers, Stock Register, Cash Book and other relevant records as required by the Assistant Director (H&T).' vs File2='Documents RequiredResidence Proof of Assam StateProof of IdentityIncome CertificateBank Account DetailsPassport Size PhotographLatest Salary SlipsStatement of Salary Account for the Past Six MonthsNo Dues Salary From the Existing Banker if the Salary is Credited in Other Than SBI AccountProof of Being in Service for a Minimum of Five YearsStatement of Personal Assets and Liabilities in Banks FormatDocuments Evidencing the Ownership of Land'\n",
      "Row 0, Column 'FAQ': File1='Frequently Asked QuestionsWhich Department Manages This Scheme?This scheme is managed by the Department of Handlooms & Textiles, Government of Andhra Pradesh.Is This Scheme State Sponsored Or Centrally Sponsored?This scheme is a 100% State Sponsored Scheme.Where Can I Find The Link To The Original Scheme Guidelines?The Scheme Guidelines can be found at this link - https://www.aphandtex.gov.in/open_record.php?ID=136Where Can I Find The Link To The Amended Scheme Guidelines?The Scheme Guidelines can be found at this link - https://handlooms.ap.gov.in/documents/G_O_55%20Yarn_subsidy.PDFWhat Is The Full Form Of NHDC?The Full Form of NHDC is \"National Handloom Development Corporation Limited\".What Is The Address Of NHDC?The address of NHDC is: Vikas Deep, 22, Station Road, Chitwapur Bhuiyan, Udaiganj, Husainganj, Lucknow, Uttar Pradesh - 226 001.What Is The Full Form Of APCO?The Full Form of APCO is \"Andhra Pradesh State Handloom Weavers Cooperative Society\".Does This Scheme Accept Online Applications?No, this scheme only accepts offline applications. The claims for subsidy shall be submitted by the Weavers Cooperative Societies to the Assistant Director (H&T) in the prescribed proforma.What Is The URL Of The Website Of Department Of Handlooms & Textiles, Government Of Andhra Pradesh.?The URL of the website of Department of Handlooms & Textiles is https://www.aphandtex.gov.in/home.php.Is There Any Application Fee?No. The entire application process is completely free of cost.Is There A Percentage Of Slots Reserved For Female Applicants?No, there is no reservation of slots based on the gender of the applicant.For What Purposes Can The Amount Of Subsidy Be Utilized?The amount of Subsidy should be utilized for:\n",
      "1. as production bonus basing on the wages earned by the members of the concerned primary Weavers Cooperative Societies.\n",
      "2. rebate on sales or any other production related purpose.What Percentage Of The Amount Of Subsidy Can Be Utilized As Production Bonus Basing On The Wages Earned By The Members?75% of the amount of subsidy can be utilized as production bonus basing on the wages earned by the members of the concerned primary Weavers Cooperative Societies.What Percentage Of The Amount Of Subsidy Can Be Utilized As Rebate On Sales Or Any Other Production Related Purpose?25% of the amount of subsidy can be utilized as rebate on sales or any other production related purpose with effect from 1st Dec 2016.What Is The Full Form Of AD (H&T)?The Full Form of AD (H&T) is Assistant Director (Department of Handlooms & Textiles).Is There A Deadline For The Submission Of The Claims For Subsidy?Yes, the claims for subsidy shall be submitted by the Weavers Cooperative Societies to the Assistant Director (H&T) in the prescribed proforma on quarterly basis during the financial year within 15 days from the end of the respective quarter.Is There A Deadline For The AD(H&T) For Recommending The Claims?Yes, the AD(H&T) after scrutiny shall recommend the claims with in next 15 days.What Shall Be The Case If The Claims Are Found To Be Bleated?Appropriate cut will be imposed on bleated claims.Since When Is This Scheme Under Operation?The scheme is operative from 29th April 2011.Can The Weavers Coop Societies Claim Any Subsidy On The Yarn Purchased From Private Yarn Dealers?No, the Weavers Coop Societies shall NOT claim any subsidy on the Yarn, Dyes and Chemicals purchased from private yarn dealers, Mills etc.By When Should The Claim For Subsidy Reach Head Office?The claims for subsidy in complete shape should reach Head Office with in 30 days from the end of the Quarter.' vs File2='Frequently Asked QuestionsWhich Department Manages The \"Aponar Apon Ghar\" Home Loan Subsidy Scheme?The \"Aponar Apon Ghar\" Home Loan Subsidy Scheme is managed by the Finance Department, Govt. of Assam.What Are The Objectives Of \"Aponar Apon Ghar\"?The objective is to realize the vision of Housing For All by 2022 so that each poor person may have their own house. All the applicants will get home loans at subsidized interest rates. Should The Applicant Avail Of The Loan From A Regional Rural Bank Only, In Order To Be Eligible For The Loan Subsidy?No, the applicant can avail of housing loans from any Scheduled Commercial Bank, Regional Rural Bank, or Assam Cooperative Apex bank within the state.What Is The Maximum Amount Of Subsidy That The Applicant Can Receive On The Home Loan?The state govt. will provide  2,50,000 subsidy on home loans up to  40,00,000.Can A Person Get House Loan Subsidy For Purchasing Their 2nd House?No, this house loan subsidy is only for those who are purchasing their 1st house and have not availed of loans under the previous Apun Ghar Scheme.Can Residents From Nagaland Also Apply To This Scheme?No, the scope of the scheme is limited to the permanent residents of the state of Assam.How Much Shall Be The Subsidy On A Loan Amount Of  7,50,000?The subsidy shall be  1,00,000.How Much Shall Be The Subsidy On A Loan Amount Of  19,00,000?The subsidy shall be  1,50,000.How Much Shall Be The Subsidy On A Loan Amount Of  19,00,000?The subsidy shall be  1,50,000.Where Can I Find The Link To The Scheme Guidelines?The Scheme Guidelines can be found at this link - \n",
      "https://finance.assam.gov.in/sites/default/files/INTEREST%20SUBVENTION%20SCHEME%20FOR%20HOUSING%20LOAN%20FOR%20REGULAR%20STATE%20GOVERNMENT%20EMPLOYEES%20UNDER%20APUN%20GHAR%2C%20FILENO.%20FM.45.2016.157%2C%20DTD.21.03.2017_1.PDFWhere Can I Fin The Document Checklist?The Document Checklist can be found at this link - \n",
      "https://sivasagar.assam.gov.in/sites/default/files/public_utility/APPLICATION%20FORM%20FOR%20HOME%20LOAN_2.pdfCan I Apply Ot This Scheme If The Total Income Of My Family Is  27,00,000?No, in order to be eligible, the total family income of the applicant (from all sources) must not exceed  20,00,000.What Is The Full Form Of NPA?The Full Form of NPA is \"Non-Performing Assets\".Can I Apply Offline To This Scheme?No, this scheme only accepts Online Applications.'\n",
      "Row 0, Column 'Link': File1='https://www.myscheme.gov.in/schemes/40shydcs' vs File2='https://www.myscheme.gov.in/schemes/aag'\n",
      "Row 0, Column 'Keywords': File1='HandloomSubsidyWeaver' vs File2='LoanSubsidy'\n",
      "... and 20495 more differences.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compare_csv_files(file1, file2, max_differences=10):\n",
    "    # Read the CSV files\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "\n",
    "    # Check if the shapes of the dataframes are the same\n",
    "    if df1.shape != df2.shape:\n",
    "        print(\"The files have different shapes.\")\n",
    "        return\n",
    "\n",
    "    # Check for differences\n",
    "    differences = (df1 != df2)\n",
    "    \n",
    "    # Find differing cells\n",
    "    differing_cells = differences.stack()\n",
    "    differing_cells = differing_cells[differing_cells].index.tolist()\n",
    "    \n",
    "    total_differences = len(differing_cells)\n",
    "    if total_differences == 0:\n",
    "        print(\"The contents of the two CSV files are the same.\")\n",
    "    else:\n",
    "        print(f\"The files differ at {total_differences} positions.\")\n",
    "        print(\"Showing the first few differences:\")\n",
    "        for i, (row, col) in enumerate(differing_cells):\n",
    "            if i >= max_differences:\n",
    "                break\n",
    "            print(f\"Row {row}, Column '{col}': File1='{df1.at[row, col]}' vs File2='{df2.at[row, col]}'\")\n",
    "        if total_differences > max_differences:\n",
    "            print(f\"... and {total_differences - max_differences} more differences.\")\n",
    "\n",
    "\n",
    "compare_csv_files(file1, file2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15b4ee13-4a6e-4626-93d2-0e43ae84f67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted CSV saved to sorted_scheme_sample2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def sort_csv_by_first_column(input_file, output_file):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Check if DataFrame is empty or has no columns\n",
    "    if df.empty or df.shape[1] == 0:\n",
    "        print(\"The CSV file is empty or does not have any columns.\")\n",
    "        return\n",
    "    \n",
    "    # Sort the DataFrame by the first column\n",
    "    sorted_df = df.sort_values(by=df.columns[0])\n",
    "    \n",
    "    # Write the sorted DataFrame to a new CSV file\n",
    "    sorted_df.to_csv(output_file, index=False)\n",
    "    print(f\"Sorted CSV saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "input_file = 'scheme_sample2.csv'\n",
    "output_file = 'sorted_scheme_sample2.csv'\n",
    "sort_csv_by_first_column(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d64492b-3293-4749-affa-81a4c302a32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages or error navigating to the next page. Stopping.\n",
      "Scraped 170 links.\n"
     ]
    }
   ],
   "source": [
    "#Headless mode\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.headless = True  # Run in headless mode\n",
    "driver = webdriver.Chrome(options=options)\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "def extract_links_from_page():\n",
    "    links = []\n",
    "    divs = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.p-4.lg\\\\:p-8.w-full\")))\n",
    "    for div in divs:\n",
    "        try:\n",
    "            a_tag = div.find_element(By.XPATH, \".//h2/a\")\n",
    "            link = a_tag.get_attribute(\"href\")\n",
    "            if link:\n",
    "                links.append(link)\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding link in div: {e}\")\n",
    "    return links\n",
    "\n",
    "# Open the initial URL\n",
    "driver.get(\"https://www.myscheme.gov.in/search\")\n",
    "\n",
    "all_links_headless = []\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        all_links_headless.extend(extract_links_from_page())\n",
    "        try:\n",
    "            next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".ml-2.text-darkblue-900.dark\\\\:text-white.cursor-pointer\")))\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "            time.sleep(1)  # Wait for scrolling to complete\n",
    "            next_button.click()\n",
    "            time.sleep(2)  # Adjust the sleep time if needed to allow the next page to load properly\n",
    "        except Exception as e:\n",
    "            print(\"No more pages or error navigating to the next page. Stopping.\")\n",
    "            break\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "print(f\"Scraped {len(all_links_headless)} links.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce5c5bca-c03e-4839-bfb5-ddac5b621ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756a92a8-87a3-4605-b8df-fcdfaaad67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimization by Scraping in a Single Session\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Initialize WebDriver\n",
    "def init_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.headless = True  # Run in headless mode\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    return driver, wait\n",
    "\n",
    "def extract_links_from_page(driver, wait):\n",
    "    links = []\n",
    "    divs = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.p-4.lg\\\\:p-8.w-full\")))\n",
    "    for div in divs:\n",
    "        try:\n",
    "            a_tag = div.find_element(By.XPATH, \".//h2/a\")\n",
    "            link = a_tag.get_attribute(\"href\")\n",
    "            if link:\n",
    "                links.append(link)\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding link in div: {e}\")\n",
    "    return links\n",
    "\n",
    "def scrape_links():\n",
    "    driver, wait = init_driver()\n",
    "    all_links = []\n",
    "\n",
    "    try:\n",
    "        driver.get(\"https://www.myscheme.gov.in/search\")\n",
    "        while True:\n",
    "            all_links.extend(extract_links_from_page(driver, wait))\n",
    "            try:\n",
    "                next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".ml-2.text-darkblue-900.dark\\\\:text-white.cursor-pointer\")))\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "                time.sleep(1)  # Wait for scrolling to complete\n",
    "                next_button.click()\n",
    "                time.sleep(2)  # Adjust the sleep time if needed to allow the next page to load properly\n",
    "            except Exception as e:\n",
    "                print(\"No more pages or error navigating to the next page. Stopping.\")\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "    print(f\"Scraped {len(all_links)} links.\")\n",
    "    \n",
    "    return all_links\n",
    "\n",
    "async def fetch(session, url):\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            if response.status != 200:\n",
    "                return None\n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            scheme_name = soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\").text.strip() if soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\") else 'N/A'\n",
    "            detail = soup.find('div', id='details').text.strip() if soup.find('div', id='details') else 'N/A'\n",
    "            benefit = soup.find('div', id='benefits').text.strip() if soup.find('div', id='benefits') else 'N/A'\n",
    "            eligibility = soup.find('div', id='eligibility').text.strip() if soup.find('div', id='eligibility') else 'N/A'\n",
    "            exclusion = soup.find('div', id='exclusions').text.strip() if soup.find('div', id='exclusions') else 'N/A'\n",
    "            application_process = soup.find('div', id='application-process').text.strip() if soup.find('div', id='application-process') else 'N/A'\n",
    "            documents_required = soup.find('div', id='documents-required').text.strip() if soup.find('div', id='documents-required') else 'N/A'\n",
    "            faq = soup.find('div', id='faqs').text.strip() if soup.find('div', id='faqs') else 'N/A'\n",
    "            keyword = soup.find('div', class_='mb-2 md:mb-0 w-full').text.strip() if soup.find('div', class_='mb-2 md:mb-0 w-full') else 'N/A'\n",
    "            \n",
    "            return {\n",
    "                'SchemeName': scheme_name,\n",
    "                'Details': detail,\n",
    "                'Benefits': benefit,\n",
    "                'Eligibility': eligibility,\n",
    "                'Exclusion': exclusion,\n",
    "                'ApplicationProc': application_process,\n",
    "                'DocumentsReq': documents_required,\n",
    "                'FAQ': faq,\n",
    "                'Link': url,\n",
    "                'Keywords': keyword\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "async def scrape_scheme_pages(links):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for url in links:\n",
    "            tasks.append(fetch(session, url))\n",
    "        results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "def update_dataset(new_links):\n",
    "    # Load existing data if available\n",
    "    if os.path.exists('scheme_sample.csv'):\n",
    "        existing_data = pd.read_csv('scheme_sample.csv')\n",
    "        existing_links = set(existing_data['Link'])\n",
    "    else:\n",
    "        existing_data = pd.DataFrame()\n",
    "        existing_links = set()\n",
    "\n",
    "    # Filter out new links\n",
    "    unique_links = [link for link in new_links if link not in existing_links]\n",
    "\n",
    "    # Scrape new data\n",
    "    new_data = asyncio.run(scrape_scheme_pages(unique_links))\n",
    "    \n",
    "    # Filter out None results and create DataFrame\n",
    "    new_data = [item for item in new_data if item]\n",
    "    new_data_df = pd.DataFrame(new_data)\n",
    "\n",
    "    # Combine new data with existing data\n",
    "    if not existing_data.empty:\n",
    "        combined_data = pd.concat([existing_data, new_data_df]).drop_duplicates(subset='Link', keep='last')\n",
    "    else:\n",
    "        combined_data = new_data_df\n",
    "\n",
    "    # Save the combined data to CSV file\n",
    "    combined_data.to_csv('scheme_sample.csv', index=False)\n",
    "\n",
    "    print(\"Dataset updated successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    new_links = scrape_links()\n",
    "    update_dataset(new_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fb7f6d3f-5ea3-45a5-8d41-b6061f5703a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=127.0.6533.89)\nStacktrace:\n0   chromedriver                        0x00000001025fcee8 cxxbridge1$str$ptr + 1871728\n1   chromedriver                        0x00000001025f554c cxxbridge1$str$ptr + 1840596\n2   chromedriver                        0x000000010220882c cxxbridge1$string$len + 88532\n3   chromedriver                        0x00000001021e3e5c core::str::slice_error_fail::he7b2aa4898bc357e + 3908\n4   chromedriver                        0x0000000102271574 cxxbridge1$string$len + 517916\n5   chromedriver                        0x0000000102283ddc cxxbridge1$string$len + 593796\n6   chromedriver                        0x0000000102241478 cxxbridge1$string$len + 321056\n7   chromedriver                        0x00000001022420e8 cxxbridge1$string$len + 324240\n8   chromedriver                        0x00000001025c49fc cxxbridge1$str$ptr + 1641092\n9   chromedriver                        0x00000001025c9308 cxxbridge1$str$ptr + 1659792\n10  chromedriver                        0x00000001025aab78 cxxbridge1$str$ptr + 1534976\n11  chromedriver                        0x00000001025c9ab8 cxxbridge1$str$ptr + 1661760\n12  chromedriver                        0x000000010259ca50 cxxbridge1$str$ptr + 1477336\n13  chromedriver                        0x00000001025e6970 cxxbridge1$str$ptr + 1780216\n14  chromedriver                        0x00000001025e6aec cxxbridge1$str$ptr + 1780596\n15  chromedriver                        0x00000001025f5180 cxxbridge1$str$ptr + 1839624\n16  libsystem_pthread.dylib             0x0000000189eb5034 _pthread_start + 136\n17  libsystem_pthread.dylib             0x0000000189eafe3c thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 157\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping and updating complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[70], line 115\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Setup Selenium and extract links\u001b[39;00m\n\u001b[1;32m    114\u001b[0m driver, wait \u001b[38;5;241m=\u001b[39m setup_selenium()\n\u001b[0;32m--> 115\u001b[0m all_links \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_all_links\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Load existing data if available\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscheme_sample3.csv\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[0;32mIn[70], line 42\u001b[0m, in \u001b[0;36mscrape_all_links\u001b[0;34m(driver, wait, base_url)\u001b[0m\n\u001b[1;32m     39\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(base_url)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     all_links\u001b[38;5;241m.\u001b[39mextend(\u001b[43mextract_links_from_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m         next_button \u001b[38;5;241m=\u001b[39m wait\u001b[38;5;241m.\u001b[39muntil(EC\u001b[38;5;241m.\u001b[39melement_to_be_clickable((By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.ml-2.text-darkblue-900.dark\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m:text-white.cursor-pointer\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "Cell \u001b[0;32mIn[70], line 26\u001b[0m, in \u001b[0;36mextract_links_from_page\u001b[0;34m(driver, wait)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_links_from_page\u001b[39m(driver, wait):\n\u001b[1;32m     25\u001b[0m     links \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 26\u001b[0m     divs \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_all_elements_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCSS_SELECTOR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiv.p-4.lg\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m:p-8.w-full\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m div \u001b[38;5;129;01min\u001b[39;00m divs:\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/selenium/webdriver/support/wait.py:96\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_driver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value:\n\u001b[1;32m     98\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/selenium/webdriver/support/expected_conditions.py:191\u001b[0m, in \u001b[0;36mpresence_of_all_elements_located.<locals>._predicate\u001b[0;34m(driver)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predicate\u001b[39m(driver: WebDriverOrWebElement):\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlocator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:778\u001b[0m, in \u001b[0;36mWebDriver.find_elements\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    774\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;66;03m# Return empty list if driver returns null\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# See https://github.com/SeleniumHQ/selenium/issues/4555\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENTS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:354\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=127.0.6533.89)\nStacktrace:\n0   chromedriver                        0x00000001025fcee8 cxxbridge1$str$ptr + 1871728\n1   chromedriver                        0x00000001025f554c cxxbridge1$str$ptr + 1840596\n2   chromedriver                        0x000000010220882c cxxbridge1$string$len + 88532\n3   chromedriver                        0x00000001021e3e5c core::str::slice_error_fail::he7b2aa4898bc357e + 3908\n4   chromedriver                        0x0000000102271574 cxxbridge1$string$len + 517916\n5   chromedriver                        0x0000000102283ddc cxxbridge1$string$len + 593796\n6   chromedriver                        0x0000000102241478 cxxbridge1$string$len + 321056\n7   chromedriver                        0x00000001022420e8 cxxbridge1$string$len + 324240\n8   chromedriver                        0x00000001025c49fc cxxbridge1$str$ptr + 1641092\n9   chromedriver                        0x00000001025c9308 cxxbridge1$str$ptr + 1659792\n10  chromedriver                        0x00000001025aab78 cxxbridge1$str$ptr + 1534976\n11  chromedriver                        0x00000001025c9ab8 cxxbridge1$str$ptr + 1661760\n12  chromedriver                        0x000000010259ca50 cxxbridge1$str$ptr + 1477336\n13  chromedriver                        0x00000001025e6970 cxxbridge1$str$ptr + 1780216\n14  chromedriver                        0x00000001025e6aec cxxbridge1$str$ptr + 1780596\n15  chromedriver                        0x00000001025f5180 cxxbridge1$str$ptr + 1839624\n16  libsystem_pthread.dylib             0x0000000189eb5034 _pthread_start + 136\n17  libsystem_pthread.dylib             0x0000000189eafe3c thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "#WEB SCRAPING BASED ON LINKS\n",
    "#If link already in csv, discard\n",
    "#777 seconds \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def setup_selenium(headless=True):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.headless = True\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    return driver, wait\n",
    "\n",
    "def extract_links_from_page(driver, wait):\n",
    "    links = []\n",
    "    divs = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.p-4.lg\\\\:p-8.w-full\")))\n",
    "    for div in divs:\n",
    "        try:\n",
    "            a_tag = div.find_element(By.XPATH, \".//h2/a\")\n",
    "            link = a_tag.get_attribute(\"href\")\n",
    "            if link:\n",
    "                links.append(link)\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding link in div: {e}\")\n",
    "    return links\n",
    "\n",
    "def scrape_all_links(driver, wait, base_url):\n",
    "    all_links = []\n",
    "    driver.get(base_url)\n",
    "    \n",
    "    while True:\n",
    "        all_links.extend(extract_links_from_page(driver, wait))\n",
    "        try:\n",
    "            next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".ml-2.text-darkblue-900.dark\\\\:text-white.cursor-pointer\")))\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "            time.sleep(1)  # Wait for scrolling to complete\n",
    "            next_button.click()\n",
    "            time.sleep(2)  # Adjust the sleep time if needed to allow the next page to load properly\n",
    "        except Exception as e:\n",
    "            print(\"No more pages or error navigating to the next page. Stopping.\")\n",
    "            break\n",
    "    \n",
    "    driver.quit()\n",
    "    print(f\"Scraped {len(all_links)} links.\")\n",
    "\n",
    "    return all_links\n",
    "\n",
    "def requests_session_with_retries(retries=3, backoff_factor=1):\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=(500, 502, 503, 504),\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "def scrape_scheme_page(session, url):\n",
    "    try:\n",
    "        response = session.get(url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {url} with status code {response.status_code}\")\n",
    "            return None\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        scheme_name = soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\").text.strip() if soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\") else 'N/A'\n",
    "        detail = soup.find('div', id='details').text.strip() if soup.find('div', id='details') else 'N/A'\n",
    "        benefit = soup.find('div', id='benefits').text.strip() if soup.find('div', id='benefits') else 'N/A'\n",
    "        eligibility = soup.find('div', id='eligibility').text.strip() if soup.find('div', id='eligibility') else 'N/A'\n",
    "        exclusion = soup.find('div', id='exclusions').text.strip() if soup.find('div', id='exclusions') else 'N/A'\n",
    "        application_process = soup.find('div', id='application-process').text.strip() if soup.find('div', id='application-process') else 'N/A'\n",
    "        documents_required = soup.find('div', id='documents-required').text.strip() if soup.find('div', id='documents-required') else 'N/A'\n",
    "        faq = soup.find('div', id='faqs').text.strip() if soup.find('div', id='faqs') else 'N/A'\n",
    "        keyword = soup.find('div', class_='mb-2 md:mb-0 w-full').text.strip() if soup.find('div', class_='mb-2 md:mb-0 w-full') else 'N/A'\n",
    "        \n",
    "        return {\n",
    "            'SchemeName': scheme_name,\n",
    "            'Details': detail,\n",
    "            'Benefits': benefit,\n",
    "            'Eligibility': eligibility,\n",
    "            'Exclusion': exclusion,\n",
    "            'ApplicationProc': application_process,\n",
    "            'DocumentsReq': documents_required,\n",
    "            'FAQ': faq,\n",
    "            'Link': url,\n",
    "            'Keywords': keyword\n",
    "        }\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request error scraping {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.myscheme.gov.in/search\"\n",
    "    \n",
    "    # Setup Selenium and extract links\n",
    "    driver, wait = setup_selenium()\n",
    "    all_links = scrape_all_links(driver, wait, base_url)\n",
    "    \n",
    "    # Load existing data if available\n",
    "    if os.path.exists('scheme_sample3.csv'):\n",
    "        existing_data = pd.read_csv('scheme_sample3.csv')\n",
    "        existing_links = set(existing_data['Link'])\n",
    "    else:\n",
    "        existing_data = pd.DataFrame()\n",
    "        existing_links = set()\n",
    "    \n",
    "    unique_links = list(set(all_links) - existing_links)\n",
    "    \n",
    "    # Initialize lists to store new data\n",
    "    new_data = []\n",
    "\n",
    "    # Create a requests session with retries\n",
    "    session = requests_session_with_retries()\n",
    "\n",
    "    # Use ThreadPoolExecutor to manage concurrent scraping\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(scrape_scheme_page, session, url) for url in unique_links]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                new_data.append(result)\n",
    "\n",
    "    # Create a pandas DataFrame for new data\n",
    "    new_data_df = pd.DataFrame(new_data)\n",
    "\n",
    "    # Combine new data with existing data\n",
    "    if not existing_data.empty:\n",
    "        combined_data = pd.concat([existing_data, new_data_df]).drop_duplicates(subset='Link', keep='last')\n",
    "    else:\n",
    "        combined_data = new_data_df\n",
    "\n",
    "    # Save the combined data to CSV file\n",
    "    combined_data.to_csv('scheme_sample3.csv', index=False)\n",
    "\n",
    "    print(\"Scraping and updating complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2096af22-6638-462c-9f42-6aa5a5f0fced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch https://www.myscheme.gov.in/schemes/elsnskfdch with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/fbascs with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/scsssetul with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/icms with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/amsmhsslc with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/madp with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/spy-gbocwwb with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/aamgsiscs with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/lels with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/makssy with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/jc with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/ehstgs with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/maflap with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/cmlh-prem with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/nassvscb with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/samruddhischeme with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/dwps with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/iu-sss-ner with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/naospasa-oai with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/ipshacihh with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/maanbocwwb with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/sssbmbcascvssvs with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/iasirfa-sw with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/cmgcpsii with status code 429Failed to fetch https://www.myscheme.gov.in/schemes/swlhps with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/pseshsfdc with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/fc-giapswb with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/nhdp-iandts-emporia with status code 429\n",
      "\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/gsca with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/fmscspscc with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/sitare-gyti with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/pahal-dbtl with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/kssugbocwwb with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/wrflsncs-idra with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/btlpdfp with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/sssvamnapy with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/mbs-pulws with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/tessdl with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/auscstpaa with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/dpiit-is with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/dbhs-2bhks with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/hriday with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/mambocwwb with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/prashad with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/maftcw with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/sbssps with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/iscwr with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/pssppy with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/idcm with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/sisdgft with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/jbcmcshsfdc with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/spfappbga with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/gfapscplw with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/edftcw with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/rmewf-education with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/ses-goa with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/gobardhan with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/bps with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/wmcl with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/ad-hsb with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/upoaps with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/pmegp with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/aipbo-faascdmf with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/mkaps with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/apwesmnc with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/nsy with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/rpsy with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/psfdt with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/naospasa-bnpiocpasa with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/nassvsbulbpsfsc with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/isscpswhse with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/msssy with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/smsm with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/rtfd with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/isa with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/sasi with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/dsctassgcsccm with status code 429Failed to fetch https://www.myscheme.gov.in/schemes/chief-minister-agriculture-and-food-processing-scheme with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/naospasa-bri with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/gras with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/eicca with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/siupd with status code 429\n",
      "\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/vfmbs with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/sspftws with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/aag with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/lbssf-gtl with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/smtwoc with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/cmacs with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/midhhmneh-meghalaya with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/psftj with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/nmcms with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/dbabocwwb with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/ignvpyr with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/cmuccscs with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/nsjbsy with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/faspalscf with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/darygbocwwb with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/tnztbtsfw with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/akbhcy with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/mmsagy with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/hliss-glwb with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/pmsfscs with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/icar-ep with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/gtadap with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/issfb with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/rmewf-vocational-training with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/cels-s with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/kalias with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/mmky-fanben with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/onorc with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/sspftsc with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/sevtgi with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/nos-swd with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/fs with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/ltas with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/mfs-cl2mc with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/sebpu with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/anishi with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/fatcwd-hlwb with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/ied with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/bbsa with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/aktinf with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/ppma with status code 429\n",
      "Failed to fetch https://www.myscheme.gov.in/schemes/ltmbocwwb with status code 429\n",
      "Scraping and updating complete.\n"
     ]
    }
   ],
   "source": [
    "#checks with contents of csv\n",
    "#439 seconds\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def setup_selenium(headless=True):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.headless = True\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    return driver, wait\n",
    "\n",
    "def extract_links_from_page(driver, wait):\n",
    "    links = []\n",
    "    divs = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.p-4.lg\\\\:p-8.w-full\")))\n",
    "    for div in divs:\n",
    "        try:\n",
    "            a_tag = div.find_element(By.XPATH, \".//h2/a\")\n",
    "            link = a_tag.get_attribute(\"href\")\n",
    "            if link:\n",
    "                links.append(link)\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding link in div: {e}\")\n",
    "    return links\n",
    "\n",
    "def scrape_all_links(driver, wait, base_url):\n",
    "    all_links = []\n",
    "    driver.get(base_url)\n",
    "    \n",
    "    while True:\n",
    "        all_links.extend(extract_links_from_page(driver, wait))\n",
    "        try:\n",
    "            next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".ml-2.text-darkblue-900.dark\\\\:text-white.cursor-pointer\")))\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "            time.sleep(1)  # Wait for scrolling to complete\n",
    "            next_button.click()\n",
    "            time.sleep(2)  # Adjust the sleep time if needed to allow the next page to load properly\n",
    "        except Exception as e:\n",
    "            #print(\"No more pages or error navigating to the next page. Stopping.\")\n",
    "            break\n",
    "    \n",
    "    driver.quit()\n",
    "    return all_links\n",
    "\n",
    "def requests_session_with_retries(retries=3, backoff_factor=1):\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=(500, 502, 503, 504),\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "def scrape_scheme_page(session, url):\n",
    "    try:\n",
    "        response = session.get(url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {url} with status code {response.status_code}\")\n",
    "            return None\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        scheme_name = soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\").text.strip() if soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\") else 'N/A'\n",
    "        detail = soup.find('div', id='details').text.strip() if soup.find('div', id='details') else 'N/A'\n",
    "        benefit = soup.find('div', id='benefits').text.strip() if soup.find('div', id='benefits') else 'N/A'\n",
    "        eligibility = soup.find('div', id='eligibility').text.strip() if soup.find('div', id='eligibility') else 'N/A'\n",
    "        exclusion = soup.find('div', id='exclusions').text.strip() if soup.find('div', id='exclusions') else 'N/A'\n",
    "        application_process = soup.find('div', id='application-process').text.strip() if soup.find('div', id='application-process') else 'N/A'\n",
    "        documents_required = soup.find('div', id='documents-required').text.strip() if soup.find('div', id='documents-required') else 'N/A'\n",
    "        faq = soup.find('div', id='faqs').text.strip() if soup.find('div', id='faqs') else 'N/A'\n",
    "        keyword = soup.find('div', class_='mb-2 md:mb-0 w-full').text.strip() if soup.find('div', class_='mb-2 md:mb-0 w-full') else 'N/A'\n",
    "        \n",
    "        return {\n",
    "            'SchemeName': scheme_name,\n",
    "            'Details': detail,\n",
    "            'Benefits': benefit,\n",
    "            'Eligibility': eligibility,\n",
    "            'Exclusion': exclusion,\n",
    "            'ApplicationProc': application_process,\n",
    "            'DocumentsReq': documents_required,\n",
    "            'FAQ': faq,\n",
    "            'Link': url,\n",
    "            'Keywords': keyword\n",
    "        }\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request error scraping {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.myscheme.gov.in/search\"\n",
    "    \n",
    "    # Setup Selenium and extract links\n",
    "    driver, wait = setup_selenium()\n",
    "    all_links = scrape_all_links(driver, wait, base_url)\n",
    "    \n",
    "    # Load existing data if available\n",
    "    if os.path.exists('scheme_sample4.csv'):\n",
    "        existing_data = pd.read_csv('scheme_sample4.csv')\n",
    "        existing_data['Combined'] = existing_data.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "    else:\n",
    "        existing_data = pd.DataFrame()\n",
    "    \n",
    "    unique_links = list(set(all_links))\n",
    "    \n",
    "    # Initialize lists to store new data\n",
    "    new_data = []\n",
    "\n",
    "    # Create a requests session with retries\n",
    "    session = requests_session_with_retries()\n",
    "\n",
    "    # Use ThreadPoolExecutor to manage concurrent scraping\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(scrape_scheme_page, session, url) for url in unique_links]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                new_data.append(result)\n",
    "\n",
    "    # Create a pandas DataFrame for new data\n",
    "    new_data_df = pd.DataFrame(new_data)\n",
    "\n",
    "    if not existing_data.empty:\n",
    "        # Remove 'Combined' column from existing data\n",
    "        existing_data = existing_data.drop(columns=['Combined'])\n",
    "        \n",
    "        # Update existing entries and add new entries\n",
    "        for index, row in new_data_df.iterrows():\n",
    "            link = row['Link']\n",
    "            if link in existing_data['Link'].values:\n",
    "                # Compare the new and existing data\n",
    "                existing_row = existing_data[existing_data['Link'] == link].iloc[0]\n",
    "                combined_existing = ' '.join(existing_row.values.astype(str))\n",
    "                combined_new = ' '.join(row.values.astype(str))\n",
    "                if combined_existing != combined_new:\n",
    "                    # Update the row in existing data\n",
    "                    existing_data.update(new_data_df.loc[index])\n",
    "            else:\n",
    "                # Add new entry\n",
    "                existing_data = pd.concat([existing_data, new_data_df.loc[[index]]])\n",
    "    else:\n",
    "        existing_data = new_data_df\n",
    "\n",
    "    # Save the updated data to CSV file\n",
    "    existing_data.to_csv('scheme_sample4.csv', index=False)\n",
    "\n",
    "    print(\"Scraping and updating complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b358d967-704d-4406-a53d-dd162cc7f475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages or error navigating to the next page. Stopping.\n",
      "Scraped 10 links.\n",
      "Scraping and updating complete.\n"
     ]
    }
   ],
   "source": [
    "#480 seconds\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def setup_selenium(headless=True):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.headless = True\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    return driver, wait\n",
    "\n",
    "def extract_links_from_page(driver, wait):\n",
    "    links = []\n",
    "    divs = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.p-4.lg\\\\:p-8.w-full\")))\n",
    "    for div in divs:\n",
    "        try:\n",
    "            a_tag = div.find_element(By.XPATH, \".//h2/a\")\n",
    "            link = a_tag.get_attribute(\"href\")\n",
    "            if link:\n",
    "                links.append(link)\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding link in div: {e}\")\n",
    "    return links\n",
    "\n",
    "def scrape_all_links(driver, wait, base_url):\n",
    "    all_links = []\n",
    "    driver.get(base_url)\n",
    "    \n",
    "    while True:\n",
    "        all_links.extend(extract_links_from_page(driver, wait))\n",
    "        try:\n",
    "            next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".ml-2.text-darkblue-900.dark\\\\:text-white.cursor-pointer\")))\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "            time.sleep(1)  # Wait for scrolling to complete\n",
    "            next_button.click()\n",
    "            time.sleep(2)  # Adjust the sleep time if needed to allow the next page to load properly\n",
    "        except Exception as e:\n",
    "            print(\"No more pages or error navigating to the next page. Stopping.\")\n",
    "            break\n",
    "    \n",
    "    driver.quit()\n",
    "    print(f\"Scraped {len(all_links)} links.\")\n",
    "\n",
    "    return all_links\n",
    "\n",
    "def requests_session_with_retries(retries=3, backoff_factor=1):\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=(500, 502, 503, 504),\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "def scrape_scheme_page(session, url):\n",
    "    try:\n",
    "        response = session.get(url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {url} with status code {response.status_code}\")\n",
    "            return None\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        scheme_name = soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\").text.strip() if soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\") else 'N/A'\n",
    "        detail = soup.find('div', id='details').text.strip() if soup.find('div', id='details') else 'N/A'\n",
    "        benefit = soup.find('div', id='benefits').text.strip() if soup.find('div', id='benefits') else 'N/A'\n",
    "        eligibility = soup.find('div', id='eligibility').text.strip() if soup.find('div', id='eligibility') else 'N/A'\n",
    "        exclusion = soup.find('div', id='exclusions').text.strip() if soup.find('div', id='exclusions') else 'N/A'\n",
    "        application_process = soup.find('div', id='application-process').text.strip() if soup.find('div', id='application-process') else 'N/A'\n",
    "        documents_required = soup.find('div', id='documents-required').text.strip() if soup.find('div', id='documents-required') else 'N/A'\n",
    "        faq = soup.find('div', id='faqs').text.strip() if soup.find('div', id='faqs') else 'N/A'\n",
    "        keyword = soup.find('div', class_='mb-2 md:mb-0 w-full').text.strip() if soup.find('div', class_='mb-2 md:mb-0 w-full') else 'N/A'\n",
    "        \n",
    "        return {\n",
    "            'SchemeName': scheme_name,\n",
    "            'Details': detail,\n",
    "            'Benefits': benefit,\n",
    "            'Eligibility': eligibility,\n",
    "            'Exclusion': exclusion,\n",
    "            'ApplicationProc': application_process,\n",
    "            'DocumentsReq': documents_required,\n",
    "            'FAQ': faq,\n",
    "            'Link': url,\n",
    "            'Keywords': keyword\n",
    "        }\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request error scraping {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.myscheme.gov.in/search\"\n",
    "    \n",
    "    # Setup Selenium and extract links\n",
    "    driver, wait = setup_selenium()\n",
    "    all_links = scrape_all_links(driver, wait, base_url)\n",
    "    \n",
    "    # Load existing data if available\n",
    "    if os.path.exists('scheme_sample3.csv'):\n",
    "        existing_data = pd.read_csv('scheme_sample3.csv')\n",
    "        existing_links = set(existing_data['Link'])\n",
    "    else:\n",
    "        existing_data = pd.DataFrame()\n",
    "        existing_links = set()\n",
    "    \n",
    "    unique_links = list(set(all_links) - existing_links)\n",
    "    \n",
    "    # Initialize lists to store new data\n",
    "    new_data = []\n",
    "\n",
    "    # Create a requests session with retries\n",
    "    session = requests_session_with_retries()\n",
    "\n",
    "    # Use ThreadPoolExecutor to manage concurrent scraping\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(scrape_scheme_page, session, url) for url in unique_links]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                new_data.append(result)\n",
    "\n",
    "    # Create a pandas DataFrame for new data\n",
    "    new_data_df = pd.DataFrame(new_data)\n",
    "\n",
    "    # Combine new data with existing data\n",
    "    if not existing_data.empty:\n",
    "        combined_data = pd.concat([existing_data, new_data_df]).drop_duplicates()\n",
    "    else:\n",
    "        combined_data = new_data_df\n",
    "\n",
    "    # Remove older rows if two rows have the same scheme name\n",
    "    combined_data = combined_data.sort_values(by='Link').drop_duplicates(subset='SchemeName', keep='last')\n",
    "\n",
    "    # Save the combined data to CSV file\n",
    "    combined_data.to_csv('scheme_sample3.csv', index=False)\n",
    "\n",
    "    print(\"Scraping and updating complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037151c9-1153-443d-9fa3-7afe4672f665",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "68379c70-635a-4d94-a811-a1a211d92334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages or error navigating to the next page. Stopping.\n",
      "Scraped 2449 links.\n",
      "Unique 2432 links.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 178\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping and updating complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 178\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[84], line 155\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    154\u001b[0m             new_data\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[0;32m--> 155\u001b[0m             \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Random sleep to prevent hitting rate limits\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Create a pandas DataFrame for new data\u001b[39;00m\n\u001b[1;32m    158\u001b[0m new_data_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(new_data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Taking a lot of time 1000+ seconds\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import random\n",
    "\n",
    "def setup_selenium(headless=True):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.headless = True\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    return driver, wait\n",
    "\n",
    "def extract_links_from_page(driver, wait):\n",
    "    links = []\n",
    "    divs = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.p-4.lg\\\\:p-8.w-full\")))\n",
    "    for div in divs:\n",
    "        try:\n",
    "            a_tag = div.find_element(By.XPATH, \".//h2/a\")\n",
    "            link = a_tag.get_attribute(\"href\")\n",
    "            if link:\n",
    "                links.append(link)\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding link in div: {e}\")\n",
    "    return links\n",
    "\n",
    "def scrape_all_links(driver, wait, base_url):\n",
    "    all_links = []\n",
    "    driver.get(base_url)\n",
    "    \n",
    "    while True:\n",
    "        all_links.extend(extract_links_from_page(driver, wait))\n",
    "        try:\n",
    "            next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".ml-2.text-darkblue-900.dark\\\\:text-white.cursor-pointer\")))\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "            time.sleep(1)  # Wait for scrolling to complete\n",
    "            next_button.click()\n",
    "            time.sleep(2)  # Adjust the sleep time if needed to allow the next page to load properly\n",
    "        except Exception as e:\n",
    "            print(\"No more pages or error navigating to the next page. Stopping.\")\n",
    "            break\n",
    "    \n",
    "    driver.quit()\n",
    "    print(f\"Scraped {len(all_links)} links.\")\n",
    "\n",
    "    return all_links\n",
    "\n",
    "def requests_session_with_retries(retries=3, backoff_factor=1):\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=(429, 500, 502, 503, 504),\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "def scrape_scheme_page(session, url):\n",
    "    retries = 5\n",
    "    delay = 1  # initial delay in seconds\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = session.get(url, timeout=10)\n",
    "            if response.status_code == 429:\n",
    "                print(f\"Rate limit hit. Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2  # exponential backoff\n",
    "                continue\n",
    "            elif response.status_code != 200:\n",
    "                print(f\"Failed to fetch {url} with status code {response.status_code}\")\n",
    "                return None\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            scheme_name = soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\").text.strip() if soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\") else 'N/A'\n",
    "            detail = soup.find('div', id='details').text.strip() if soup.find('div', id='details') else 'N/A'\n",
    "            benefit = soup.find('div', id='benefits').text.strip() if soup.find('div', id='benefits') else 'N/A'\n",
    "            eligibility = soup.find('div', id='eligibility').text.strip() if soup.find('div', id='eligibility') else 'N/A'\n",
    "            exclusion = soup.find('div', id='exclusions').text.strip() if soup.find('div', id='exclusions') else 'N/A'\n",
    "            application_process = soup.find('div', id='application-process').text.strip() if soup.find('div', id='application-process') else 'N/A'\n",
    "            documents_required = soup.find('div', id='documents-required').text.strip() if soup.find('div', id='documents-required') else 'N/A'\n",
    "            faq = soup.find('div', id='faqs').text.strip() if soup.find('div', id='faqs') else 'N/A'\n",
    "            keyword = soup.find('div', class_='mb-2 md:mb-0 w-full').text.strip() if soup.find('div', class_='mb-2 md:mb-0 w-full') else 'N/A'\n",
    "            \n",
    "            return {\n",
    "                'SchemeName': scheme_name,\n",
    "                'Details': detail,\n",
    "                'Benefits': benefit,\n",
    "                'Eligibility': eligibility,\n",
    "                'Exclusion': exclusion,\n",
    "                'ApplicationProc': application_process,\n",
    "                'DocumentsReq': documents_required,\n",
    "                'FAQ': faq,\n",
    "                'Link': url,\n",
    "                'Keywords': keyword\n",
    "            }\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request error scraping {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.myscheme.gov.in/search\"\n",
    "    \n",
    "    # Setup Selenium and extract links\n",
    "    driver, wait = setup_selenium()\n",
    "    all_links = scrape_all_links(driver, wait, base_url)\n",
    "\n",
    "    seen = set()\n",
    "    unique_links = []\n",
    "    for item in all_links:\n",
    "        if item not in seen:\n",
    "            unique_links.append(item)\n",
    "            seen.add(item)\n",
    "\n",
    "    print(f\"Unique {len(unique_links)} links.\")\n",
    "\n",
    "    all_links = unique_links\n",
    "    \n",
    "    # Load existing data if available\n",
    "    if os.path.exists('scheme_sample3.csv'):\n",
    "        existing_data = pd.read_csv('scheme_sample3.csv')\n",
    "    else:\n",
    "        existing_data = pd.DataFrame()\n",
    "    \n",
    "    # Initialize lists to store new data\n",
    "    new_data = []\n",
    "\n",
    "    # Create a requests session with retries\n",
    "    session = requests_session_with_retries()\n",
    "\n",
    "    # Use ThreadPoolExecutor to manage concurrent scraping\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(scrape_scheme_page, session, url) for url in all_links]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                new_data.append(result)\n",
    "                time.sleep(random.uniform(1, 3))  # Random sleep to prevent hitting rate limits\n",
    "\n",
    "    # Create a pandas DataFrame for new data\n",
    "    new_data_df = pd.DataFrame(new_data)\n",
    "\n",
    "    # Combine new data with existing data\n",
    "    if not existing_data.empty:\n",
    "        combined_data = pd.concat([existing_data, new_data_df])\n",
    "    else:\n",
    "        combined_data = new_data_df\n",
    "\n",
    "    # Remove duplicates based on 'Link' and keep the latest\n",
    "    combined_data = combined_data.drop_duplicates(subset=['Link'], keep='last')\n",
    "    \n",
    "    # Remove duplicates based on 'SchemeName' and keep the latest\n",
    "    combined_data = combined_data.sort_values('Link').drop_duplicates(subset=['SchemeName'], keep='last')\n",
    "\n",
    "    # Save the combined data to CSV file\n",
    "    combined_data.to_csv('scheme_sample3.csv', index=False)\n",
    "\n",
    "    print(\"Scraping and updating complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c8e3c962-afff-497a-a579-f91bb515f5ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=127.0.6533.89)\nStacktrace:\n0   chromedriver                        0x0000000100a4cee8 cxxbridge1$str$ptr + 1871728\n1   chromedriver                        0x0000000100a4554c cxxbridge1$str$ptr + 1840596\n2   chromedriver                        0x000000010065882c cxxbridge1$string$len + 88532\n3   chromedriver                        0x0000000100633e5c core::str::slice_error_fail::he7b2aa4898bc357e + 3908\n4   chromedriver                        0x00000001006c1574 cxxbridge1$string$len + 517916\n5   chromedriver                        0x00000001006d3ddc cxxbridge1$string$len + 593796\n6   chromedriver                        0x0000000100691478 cxxbridge1$string$len + 321056\n7   chromedriver                        0x00000001006920e8 cxxbridge1$string$len + 324240\n8   chromedriver                        0x0000000100a149fc cxxbridge1$str$ptr + 1641092\n9   chromedriver                        0x0000000100a19308 cxxbridge1$str$ptr + 1659792\n10  chromedriver                        0x00000001009fab78 cxxbridge1$str$ptr + 1534976\n11  chromedriver                        0x0000000100a19ab8 cxxbridge1$str$ptr + 1661760\n12  chromedriver                        0x00000001009eca50 cxxbridge1$str$ptr + 1477336\n13  chromedriver                        0x0000000100a36970 cxxbridge1$str$ptr + 1780216\n14  chromedriver                        0x0000000100a36aec cxxbridge1$str$ptr + 1780596\n15  chromedriver                        0x0000000100a45180 cxxbridge1$str$ptr + 1839624\n16  libsystem_pthread.dylib             0x0000000189eb5034 _pthread_start + 136\n17  libsystem_pthread.dylib             0x0000000189eafe3c thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 178\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping and updating complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 178\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[91], line 122\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Setup Selenium and extract links\u001b[39;00m\n\u001b[1;32m    121\u001b[0m driver, wait \u001b[38;5;241m=\u001b[39m setup_selenium()\n\u001b[0;32m--> 122\u001b[0m all_links \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_all_links\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    125\u001b[0m unique_links \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[91], line 41\u001b[0m, in \u001b[0;36mscrape_all_links\u001b[0;34m(driver, wait, base_url)\u001b[0m\n\u001b[1;32m     38\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(base_url)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     all_links\u001b[38;5;241m.\u001b[39mextend(\u001b[43mextract_links_from_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         next_button \u001b[38;5;241m=\u001b[39m wait\u001b[38;5;241m.\u001b[39muntil(EC\u001b[38;5;241m.\u001b[39melement_to_be_clickable((By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.ml-2.text-darkblue-900.dark\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m:text-white.cursor-pointer\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "Cell \u001b[0;32mIn[91], line 25\u001b[0m, in \u001b[0;36mextract_links_from_page\u001b[0;34m(driver, wait)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_links_from_page\u001b[39m(driver, wait):\n\u001b[1;32m     24\u001b[0m     links \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 25\u001b[0m     divs \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_all_elements_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCSS_SELECTOR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiv.p-4.lg\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m:p-8.w-full\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m div \u001b[38;5;129;01min\u001b[39;00m divs:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/selenium/webdriver/support/wait.py:96\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_driver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value:\n\u001b[1;32m     98\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/selenium/webdriver/support/expected_conditions.py:191\u001b[0m, in \u001b[0;36mpresence_of_all_elements_located.<locals>._predicate\u001b[0;34m(driver)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predicate\u001b[39m(driver: WebDriverOrWebElement):\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlocator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:778\u001b[0m, in \u001b[0;36mWebDriver.find_elements\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    774\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;66;03m# Return empty list if driver returns null\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# See https://github.com/SeleniumHQ/selenium/issues/4555\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENTS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:354\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=127.0.6533.89)\nStacktrace:\n0   chromedriver                        0x0000000100a4cee8 cxxbridge1$str$ptr + 1871728\n1   chromedriver                        0x0000000100a4554c cxxbridge1$str$ptr + 1840596\n2   chromedriver                        0x000000010065882c cxxbridge1$string$len + 88532\n3   chromedriver                        0x0000000100633e5c core::str::slice_error_fail::he7b2aa4898bc357e + 3908\n4   chromedriver                        0x00000001006c1574 cxxbridge1$string$len + 517916\n5   chromedriver                        0x00000001006d3ddc cxxbridge1$string$len + 593796\n6   chromedriver                        0x0000000100691478 cxxbridge1$string$len + 321056\n7   chromedriver                        0x00000001006920e8 cxxbridge1$string$len + 324240\n8   chromedriver                        0x0000000100a149fc cxxbridge1$str$ptr + 1641092\n9   chromedriver                        0x0000000100a19308 cxxbridge1$str$ptr + 1659792\n10  chromedriver                        0x00000001009fab78 cxxbridge1$str$ptr + 1534976\n11  chromedriver                        0x0000000100a19ab8 cxxbridge1$str$ptr + 1661760\n12  chromedriver                        0x00000001009eca50 cxxbridge1$str$ptr + 1477336\n13  chromedriver                        0x0000000100a36970 cxxbridge1$str$ptr + 1780216\n14  chromedriver                        0x0000000100a36aec cxxbridge1$str$ptr + 1780596\n15  chromedriver                        0x0000000100a45180 cxxbridge1$str$ptr + 1839624\n16  libsystem_pthread.dylib             0x0000000189eb5034 _pthread_start + 136\n17  libsystem_pthread.dylib             0x0000000189eafe3c thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import random\n",
    "\n",
    "def setup_selenium(headless=True):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.headless = True\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    return driver, wait\n",
    "\n",
    "def extract_links_from_page(driver, wait):\n",
    "    links = []\n",
    "    divs = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.p-4.lg\\\\:p-8.w-full\")))\n",
    "    for div in divs:\n",
    "        try:\n",
    "            a_tag = div.find_element(By.XPATH, \".//h2/a\")\n",
    "            link = a_tag.get_attribute(\"href\")\n",
    "            if link:\n",
    "                links.append(link)\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding link in div: {e}\")\n",
    "    return links\n",
    "\n",
    "def scrape_all_links(driver, wait, base_url):\n",
    "    all_links = []\n",
    "    driver.get(base_url)\n",
    "    \n",
    "    while True:\n",
    "        all_links.extend(extract_links_from_page(driver, wait))\n",
    "        try:\n",
    "            next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".ml-2.text-darkblue-900.dark\\\\:text-white.cursor-pointer\")))\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "            time.sleep(1)  # Wait for scrolling to complete\n",
    "            next_button.click()\n",
    "            time.sleep(2)  # Adjust the sleep time if needed to allow the next page to load properly\n",
    "        except Exception as e:\n",
    "            print(\"No more pages or error navigating to the next page. Stopping.\")\n",
    "            break\n",
    "    \n",
    "    driver.quit()\n",
    "    print(f\"Scraped {len(all_links)} links.\")\n",
    "\n",
    "    return all_links\n",
    "\n",
    "def requests_session_with_retries(retries=3, backoff_factor=1):\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=(429, 500, 502, 503, 504),\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "def scrape_scheme_page(session, url):\n",
    "    retries = 5\n",
    "    delay = 1  # initial delay in seconds\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = session.get(url, timeout=10)\n",
    "            if response.status_code == 429:\n",
    "                print(f\"Rate limit hit. Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2  # exponential backoff\n",
    "                continue\n",
    "            elif response.status_code != 200:\n",
    "                print(f\"Failed to fetch {url} with status code {response.status_code}\")\n",
    "                return None\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            scheme_name = soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\").text.strip() if soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\") else 'N/A'\n",
    "            detail = soup.find('div', id='details').text.strip() if soup.find('div', id='details') else 'N/A'\n",
    "            benefit = soup.find('div', id='benefits').text.strip() if soup.find('div', id='benefits') else 'N/A'\n",
    "            eligibility = soup.find('div', id='eligibility').text.strip() if soup.find('div', id='eligibility') else 'N/A'\n",
    "            exclusion = soup.find('div', id='exclusions').text.strip() if soup.find('div', id='exclusions') else 'N/A'\n",
    "            application_process = soup.find('div', id='application-process').text.strip() if soup.find('div', id='application-process') else 'N/A'\n",
    "            documents_required = soup.find('div', id='documents-required').text.strip() if soup.find('div', id='documents-required') else 'N/A'\n",
    "            faq = soup.find('div', id='faqs').text.strip() if soup.find('div', id='faqs') else 'N/A'\n",
    "            keyword = soup.find('div', class_='mb-2 md:mb-0 w-full').text.strip() if soup.find('div', class_='mb-2 md:mb-0 w-full') else 'N/A'\n",
    "            \n",
    "            return {\n",
    "                'SchemeName': scheme_name,\n",
    "                'Details': detail,\n",
    "                'Benefits': benefit,\n",
    "                'Eligibility': eligibility,\n",
    "                'Exclusion': exclusion,\n",
    "                'ApplicationProc': application_process,\n",
    "                'DocumentsReq': documents_required,\n",
    "                'FAQ': faq,\n",
    "                'Link': url,\n",
    "                'Keywords': keyword\n",
    "            }\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request error scraping {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.myscheme.gov.in/search\"\n",
    "    \n",
    "    # Setup Selenium and extract links\n",
    "    driver, wait = setup_selenium()\n",
    "    all_links = scrape_all_links(driver, wait, base_url)\n",
    "\n",
    "    seen = set()\n",
    "    unique_links = []\n",
    "    for item in all_links:\n",
    "        if item not in seen:\n",
    "            unique_links.append(item)\n",
    "            seen.add(item)\n",
    "\n",
    "    print(f\"Unique {len(unique_links)} links.\")\n",
    "\n",
    "    all_links = unique_links\n",
    "    \n",
    "    # Load existing data if available\n",
    "    if os.path.exists('scheme_sample3.csv'):\n",
    "        existing_data = pd.read_csv('scheme_sample3.csv')\n",
    "    else:\n",
    "        existing_data = pd.DataFrame()\n",
    "    \n",
    "    # Initialize lists to store new data\n",
    "    new_data = []\n",
    "\n",
    "    # Create a requests session with retries\n",
    "    session = requests_session_with_retries()\n",
    "\n",
    "    # Use ThreadPoolExecutor to manage concurrent scraping\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(scrape_scheme_page, session, url) for url in all_links]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                new_data.append(result)\n",
    "                time.sleep(random.uniform(1, 3))  # Random sleep to prevent hitting rate limits\n",
    "\n",
    "    # Create a pandas DataFrame for new data\n",
    "    new_data_df = pd.DataFrame(new_data)\n",
    "\n",
    "    # Combine new data with existing data\n",
    "    if not existing_data.empty:\n",
    "        combined_data = pd.concat([existing_data, new_data_df])\n",
    "    else:\n",
    "        combined_data = new_data_df\n",
    "\n",
    "    # Remove duplicates based on 'Link' and keep the latest\n",
    "    combined_data = combined_data.drop_duplicates(subset=['Link'], keep='last')\n",
    "    \n",
    "    # Remove duplicates based on 'SchemeName' and keep the latest\n",
    "    combined_data = combined_data.sort_values('Link').drop_duplicates(subset=['SchemeName'], keep='last')\n",
    "\n",
    "    # Save the combined data to CSV file\n",
    "    combined_data.to_csv('scheme_sample3.csv', index=False)\n",
    "\n",
    "    print(\"Scraping and updating complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d2f4a9c8-9898-4044-a679-8499a4b65fb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=127.0.6533.89)\nStacktrace:\n0   chromedriver                        0x00000001044b8ee8 cxxbridge1$str$ptr + 1871728\n1   chromedriver                        0x00000001044b154c cxxbridge1$str$ptr + 1840596\n2   chromedriver                        0x00000001040c482c cxxbridge1$string$len + 88532\n3   chromedriver                        0x000000010409fe5c core::str::slice_error_fail::he7b2aa4898bc357e + 3908\n4   chromedriver                        0x000000010412d574 cxxbridge1$string$len + 517916\n5   chromedriver                        0x000000010413fddc cxxbridge1$string$len + 593796\n6   chromedriver                        0x00000001040fd478 cxxbridge1$string$len + 321056\n7   chromedriver                        0x00000001040fe0e8 cxxbridge1$string$len + 324240\n8   chromedriver                        0x00000001044809fc cxxbridge1$str$ptr + 1641092\n9   chromedriver                        0x0000000104485308 cxxbridge1$str$ptr + 1659792\n10  chromedriver                        0x0000000104466b78 cxxbridge1$str$ptr + 1534976\n11  chromedriver                        0x0000000104485ab8 cxxbridge1$str$ptr + 1661760\n12  chromedriver                        0x0000000104458a50 cxxbridge1$str$ptr + 1477336\n13  chromedriver                        0x00000001044a2970 cxxbridge1$str$ptr + 1780216\n14  chromedriver                        0x00000001044a2aec cxxbridge1$str$ptr + 1780596\n15  chromedriver                        0x00000001044b1180 cxxbridge1$str$ptr + 1839624\n16  libsystem_pthread.dylib             0x0000000189eb5034 _pthread_start + 136\n17  libsystem_pthread.dylib             0x0000000189eafe3c thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 181\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping and updating complete. Data saved to scheme_sample3.csv.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 181\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[109], line 122\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Setup Selenium and extract links\u001b[39;00m\n\u001b[1;32m    121\u001b[0m driver, wait \u001b[38;5;241m=\u001b[39m setup_selenium()\n\u001b[0;32m--> 122\u001b[0m all_links \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_all_links\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    125\u001b[0m unique_links \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[109], line 41\u001b[0m, in \u001b[0;36mscrape_all_links\u001b[0;34m(driver, wait, base_url)\u001b[0m\n\u001b[1;32m     38\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(base_url)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     all_links\u001b[38;5;241m.\u001b[39mextend(\u001b[43mextract_links_from_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         next_button \u001b[38;5;241m=\u001b[39m wait\u001b[38;5;241m.\u001b[39muntil(EC\u001b[38;5;241m.\u001b[39melement_to_be_clickable((By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.ml-2.text-darkblue-900.dark\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m:text-white.cursor-pointer\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "Cell \u001b[0;32mIn[109], line 25\u001b[0m, in \u001b[0;36mextract_links_from_page\u001b[0;34m(driver, wait)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_links_from_page\u001b[39m(driver, wait):\n\u001b[1;32m     24\u001b[0m     links \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 25\u001b[0m     divs \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_all_elements_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCSS_SELECTOR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiv.p-4.lg\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m:p-8.w-full\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m div \u001b[38;5;129;01min\u001b[39;00m divs:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/selenium/webdriver/support/wait.py:96\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_driver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value:\n\u001b[1;32m     98\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/selenium/webdriver/support/expected_conditions.py:191\u001b[0m, in \u001b[0;36mpresence_of_all_elements_located.<locals>._predicate\u001b[0;34m(driver)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predicate\u001b[39m(driver: WebDriverOrWebElement):\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlocator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:778\u001b[0m, in \u001b[0;36mWebDriver.find_elements\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    774\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;66;03m# Return empty list if driver returns null\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# See https://github.com/SeleniumHQ/selenium/issues/4555\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENTS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:354\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=127.0.6533.89)\nStacktrace:\n0   chromedriver                        0x00000001044b8ee8 cxxbridge1$str$ptr + 1871728\n1   chromedriver                        0x00000001044b154c cxxbridge1$str$ptr + 1840596\n2   chromedriver                        0x00000001040c482c cxxbridge1$string$len + 88532\n3   chromedriver                        0x000000010409fe5c core::str::slice_error_fail::he7b2aa4898bc357e + 3908\n4   chromedriver                        0x000000010412d574 cxxbridge1$string$len + 517916\n5   chromedriver                        0x000000010413fddc cxxbridge1$string$len + 593796\n6   chromedriver                        0x00000001040fd478 cxxbridge1$string$len + 321056\n7   chromedriver                        0x00000001040fe0e8 cxxbridge1$string$len + 324240\n8   chromedriver                        0x00000001044809fc cxxbridge1$str$ptr + 1641092\n9   chromedriver                        0x0000000104485308 cxxbridge1$str$ptr + 1659792\n10  chromedriver                        0x0000000104466b78 cxxbridge1$str$ptr + 1534976\n11  chromedriver                        0x0000000104485ab8 cxxbridge1$str$ptr + 1661760\n12  chromedriver                        0x0000000104458a50 cxxbridge1$str$ptr + 1477336\n13  chromedriver                        0x00000001044a2970 cxxbridge1$str$ptr + 1780216\n14  chromedriver                        0x00000001044a2aec cxxbridge1$str$ptr + 1780596\n15  chromedriver                        0x00000001044b1180 cxxbridge1$str$ptr + 1839624\n16  libsystem_pthread.dylib             0x0000000189eb5034 _pthread_start + 136\n17  libsystem_pthread.dylib             0x0000000189eafe3c thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "# works well but takes 30 minutes\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import random\n",
    "\n",
    "def setup_selenium(headless=True):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.headless = True\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    return driver, wait\n",
    "\n",
    "def extract_links_from_page(driver, wait):\n",
    "    links = []\n",
    "    divs = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.p-4.lg\\\\:p-8.w-full\")))\n",
    "    for div in divs:\n",
    "        try:\n",
    "            a_tag = div.find_element(By.XPATH, \".//h2/a\")\n",
    "            link = a_tag.get_attribute(\"href\")\n",
    "            if link:\n",
    "                links.append(link)\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding link in div: {e}\")\n",
    "    return links\n",
    "\n",
    "def scrape_all_links(driver, wait, base_url):\n",
    "    all_links = []\n",
    "    driver.get(base_url)\n",
    "    \n",
    "    while True:\n",
    "        all_links.extend(extract_links_from_page(driver, wait))\n",
    "        try:\n",
    "            next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".ml-2.text-darkblue-900.dark\\\\:text-white.cursor-pointer\")))\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "            time.sleep(0.5)  # Wait for scrolling to complete\n",
    "            next_button.click()\n",
    "            time.sleep(1.5)  # Adjust the sleep time if needed to allow the next page to load properly\n",
    "        except Exception as e:\n",
    "            print(\"No more pages or error navigating to the next page. Stopping.\")\n",
    "            break\n",
    "    \n",
    "    driver.quit()\n",
    "    print(f\"Scraped {len(all_links)} links.\")\n",
    "\n",
    "    return all_links\n",
    "\n",
    "def requests_session_with_retries(retries=3, backoff_factor=1):\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=(429, 500, 502, 503, 504),\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "def scrape_scheme_page(session, url):\n",
    "    retries = 5\n",
    "    delay = 1  # initial delay in seconds\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = session.get(url, timeout=10)\n",
    "            if response.status_code == 429:\n",
    "                print(f\"Rate limit hit. Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2  # exponential backoff\n",
    "                continue\n",
    "            elif response.status_code != 200:\n",
    "                print(f\"Failed to fetch {url} with status code {response.status_code}\")\n",
    "                return None\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            scheme_name = soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\").text.strip() if soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\") else 'N/A'\n",
    "            detail = soup.find('div', id='details').text.strip() if soup.find('div', id='details') else 'N/A'\n",
    "            benefit = soup.find('div', id='benefits').text.strip() if soup.find('div', id='benefits') else 'N/A'\n",
    "            eligibility = soup.find('div', id='eligibility').text.strip() if soup.find('div', id='eligibility') else 'N/A'\n",
    "            exclusion = soup.find('div', id='exclusions').text.strip() if soup.find('div', id='exclusions') else 'N/A'\n",
    "            application_process = soup.find('div', id='application-process').text.strip() if soup.find('div', id='application-process') else 'N/A'\n",
    "            documents_required = soup.find('div', id='documents-required').text.strip() if soup.find('div', id='documents-required') else 'N/A'\n",
    "            faq = soup.find('div', id='faqs').text.strip() if soup.find('div', id='faqs') else 'N/A'\n",
    "            keyword = soup.find('div', class_='mb-2 md:mb-0 w-full').text.strip() if soup.find('div', class_='mb-2 md:mb-0 w-full') else 'N/A'\n",
    "            \n",
    "            return {\n",
    "                'SchemeName': scheme_name,\n",
    "                'Details': detail,\n",
    "                'Benefits': benefit,\n",
    "                'Eligibility': eligibility,\n",
    "                'Exclusion': exclusion,\n",
    "                'ApplicationProc': application_process,\n",
    "                'DocumentsReq': documents_required,\n",
    "                'FAQ': faq,\n",
    "                'Link': url,\n",
    "                'Keywords': keyword\n",
    "            }\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request error scraping {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.myscheme.gov.in/search\"\n",
    "    \n",
    "    # Setup Selenium and extract links\n",
    "    driver, wait = setup_selenium()\n",
    "    all_links = scrape_all_links(driver, wait, base_url)\n",
    "\n",
    "    seen = set()\n",
    "    unique_links = []\n",
    "    for item in all_links:\n",
    "        if item not in seen:\n",
    "            unique_links.append(item)\n",
    "            seen.add(item)\n",
    "\n",
    "    print(f\"Unique {len(unique_links)} links.\")\n",
    "\n",
    "    all_links = unique_links\n",
    "\n",
    "    all_links.sort()\n",
    "    \n",
    "    # Load existing data if available\n",
    "    if os.path.exists('scheme_sample3.csv'):\n",
    "        existing_data = pd.read_csv('scheme_sample3.csv')\n",
    "    else:\n",
    "        existing_data = pd.DataFrame()\n",
    "    \n",
    "    # Initialize lists to store new data\n",
    "    new_data = []\n",
    "\n",
    "    # Create a requests session with retries\n",
    "    session = requests_session_with_retries()\n",
    "\n",
    "    # Use ThreadPoolExecutor to manage concurrent scraping\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(scrape_scheme_page, session, url) for url in all_links]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                new_data.append(result)\n",
    "                print(f\"Scraped data from {result['Link']}\")\n",
    "                time.sleep(random.uniform(1, 3))  # Random sleep to prevent hitting rate limits\n",
    "\n",
    "    # Create a pandas DataFrame for new data\n",
    "    new_data_df = pd.DataFrame(new_data)\n",
    "    print(f\"Scraped data for {len(new_data_df)} schemes.\")\n",
    "\n",
    "    # Combine new data with existing data\n",
    "    if not existing_data.empty:\n",
    "        combined_data = pd.concat([existing_data, new_data_df])\n",
    "    else:\n",
    "        combined_data = new_data_df\n",
    "\n",
    "    # Remove duplicates and keep the latest\n",
    "    combined_data = combined_data.drop_duplicates(keep='last')\n",
    "    \n",
    "    # Remove duplicates based on 'SchemeName' and keep the latest\n",
    "    combined_data = combined_data.sort_values('Link').drop_duplicates(subset=['SchemeName'], keep='last')\n",
    "\n",
    "    # Save the combined data to CSV file\n",
    "    combined_data.to_csv('scheme_sample3.csv', index=False)\n",
    "    print(\"Scraping and updating complete. Data saved to scheme_sample3.csv.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c5b790-7fd0-457f-ad11-f05297607192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import random\n",
    "\n",
    "def setup_selenium(headless=True):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.headless = True\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    return driver, wait\n",
    "    \n",
    "def extract_links_from_page(driver, wait):\n",
    "    links = []\n",
    "    divs = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.p-4.lg\\\\:p-8.w-full\")))\n",
    "    for div in divs:\n",
    "        try:\n",
    "            a_tag = div.find_element(By.XPATH, \".//h2/a\")\n",
    "            link = a_tag.get_attribute(\"href\")\n",
    "            if link:\n",
    "                links.append(link)\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding link in div: {e}\")\n",
    "    return links\n",
    "\n",
    "def scrape_all_links(driver, wait, base_url):\n",
    "    all_links = []\n",
    "    driver.get(base_url)\n",
    "    \n",
    "    while True:\n",
    "        all_links.extend(extract_links_from_page(driver, wait))\n",
    "        try:\n",
    "            next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".ml-2.text-darkblue-900.dark\\\\:text-white.cursor-pointer\")))\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "            time.sleep(0.5)  # Wait for scrolling to complete\n",
    "            next_button.click()\n",
    "            time.sleep(1.5)  # Adjust the sleep time if needed to allow the next page to load properly\n",
    "        except Exception as e:\n",
    "            #print(\"No more pages or error navigating to the next page. Stopping.\")\n",
    "            break\n",
    "    \n",
    "    driver.quit()\n",
    "    print(f\"Scraped {len(all_links)} links.\")\n",
    "    return list(set(all_links))  # Ensure uniqueness\n",
    "\n",
    "def requests_session_with_retries(retries=3, backoff_factor=1):\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=(429, 500, 502, 503, 504),\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "def scrape_scheme_page(session, url):\n",
    "    retries = 5\n",
    "    delay = 1  # initial delay in seconds\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = session.get(url, timeout=10)\n",
    "            if response.status_code == 429:\n",
    "                print(f\"Rate limit hit. Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2  # exponential backoff\n",
    "                continue\n",
    "            elif response.status_code != 200:\n",
    "                print(f\"Failed to fetch {url} with status code {response.status_code}\")\n",
    "                return None\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            scheme_name = soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\").text.strip() if soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\") else 'N/A'\n",
    "            detail = soup.find('div', id='details').text.strip() if soup.find('div', id='details') else 'N/A'\n",
    "            benefit = soup.find('div', id='benefits').text.strip() if soup.find('div', id='benefits') else 'N/A'\n",
    "            eligibility = soup.find('div', id='eligibility').text.strip() if soup.find('div', id='eligibility') else 'N/A'\n",
    "            exclusion = soup.find('div', id='exclusions').text.strip() if soup.find('div', id='exclusions') else 'N/A'\n",
    "            application_process = soup.find('div', id='application-process').text.strip() if soup.find('div', id='application-process') else 'N/A'\n",
    "            documents_required = soup.find('div', id='documents-required').text.strip() if soup.find('div', id='documents-required') else 'N/A'\n",
    "            faq = soup.find('div', id='faqs').text.strip() if soup.find('div', id='faqs') else 'N/A'\n",
    "            keyword = soup.find('div', class_='mb-2 md:mb-0 w-full').text.strip() if soup.find('div', class_='mb-2 md:mb-0 w-full') else 'N/A'\n",
    "            \n",
    "            return {\n",
    "                'SchemeName': scheme_name,\n",
    "                'Details': detail,\n",
    "                'Benefits': benefit,\n",
    "                'Eligibility': eligibility,\n",
    "                'Exclusion': exclusion,\n",
    "                'ApplicationProc': application_process,\n",
    "                'DocumentsReq': documents_required,\n",
    "                'FAQ': faq,\n",
    "                'Link': url,\n",
    "                'Keywords': keyword\n",
    "            }\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request error scraping {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.myscheme.gov.in/search\"\n",
    "    \n",
    "    # Setup Selenium and extract links\n",
    "    driver, wait = setup_selenium()\n",
    "    all_links = scrape_all_links(driver, wait, base_url)\n",
    "\n",
    "    print(f\"Unique {len(all_links)} links.\")\n",
    "\n",
    "    # Load existing data if available\n",
    "    if os.path.exists('scheme_sample3.csv'):\n",
    "        existing_data = pd.read_csv('scheme_sample3.csv')\n",
    "    else:\n",
    "        existing_data = pd.DataFrame()\n",
    "    \n",
    "    # Initialize lists to store new data\n",
    "    new_data = []\n",
    "\n",
    "    # Create a requests session with retries\n",
    "    session = requests_session_with_retries()\n",
    "\n",
    "    # Use ThreadPoolExecutor to manage concurrent scraping\n",
    "    with ThreadPoolExecutor(max_workers=20) as executor:  # Increased workers for faster scraping\n",
    "        futures = [executor.submit(scrape_scheme_page, session, url) for url in all_links]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                new_data.append(result)\n",
    "                print(f\"Scraped data from {result['Link']}\")\n",
    "                time.sleep(random.uniform(0.5, 1.5))  # Reduced sleep to speed up processing\n",
    "\n",
    "    # Create a pandas DataFrame for new data\n",
    "    new_data_df = pd.DataFrame(new_data)\n",
    "    print(f\"Scraped data for {len(new_data_df)} schemes.\")\n",
    "\n",
    "    # Combine new data with existing data\n",
    "    if not existing_data.empty:\n",
    "        combined_data = pd.concat([existing_data, new_data_df])\n",
    "    else:\n",
    "        combined_data = new_data_df\n",
    "\n",
    "    # Remove duplicates and keep the latest\n",
    "    combined_data = combined_data.drop_duplicates(keep='last')\n",
    "    \n",
    "    # Remove duplicates based on 'SchemeName' and keep the latest\n",
    "    combined_data = combined_data.sort_values('Link').drop_duplicates(subset=['SchemeName'], keep='last')\n",
    "\n",
    "    # Save the combined data to CSV file\n",
    "    combined_data.to_csv('scheme_sample3.csv', index=False)\n",
    "    print(\"Scraping and updating complete. Data saved to scheme_sample3.csv.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49305d54-fbea-4b89-a5fd-189b28ad46b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import random\n",
    "\n",
    "def setup_selenium(headless=True):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    return driver, wait\n",
    "\n",
    "def extract_links_from_page(driver, wait):\n",
    "    links = []\n",
    "    divs = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.p-4.lg\\\\:p-8.w-full\")))\n",
    "    for div in divs:\n",
    "        try:\n",
    "            a_tag = div.find_element(By.XPATH, \".//h2/a\")\n",
    "            link = a_tag.get_attribute(\"href\")\n",
    "            if link:\n",
    "                links.append(link)\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding link in div: {e}\")\n",
    "    return links\n",
    "\n",
    "def scrape_all_links(driver, wait, base_url):\n",
    "    all_links = []\n",
    "    driver.get(base_url)\n",
    "    \n",
    "    while True:\n",
    "        all_links.extend(extract_links_from_page(driver, wait))\n",
    "        try:\n",
    "            next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".ml-2.text-darkblue-900.dark\\\\:text-white.cursor-pointer\")))\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "            time.sleep(0.5)  # Wait for scrolling to complete\n",
    "            next_button.click()\n",
    "            time.sleep(1.5)  # Adjust the sleep time if needed to allow the next page to load properly\n",
    "            print(3)\n",
    "        except Exception as e:\n",
    "            #print(\"No more pages or error navigating to the next page. Stopping.\")\n",
    "            break\n",
    "    \n",
    "    driver.quit()\n",
    "    print(f\"Scraped {len(all_links)} links.\")\n",
    "    return list(set(all_links))  # Ensure uniqueness\n",
    "\n",
    "def requests_session_with_retries(retries=3, backoff_factor=1):\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=(429, 500, 502, 503, 504),\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "def scrape_scheme_page(session, url):\n",
    "    retries = 5\n",
    "    delay = 1  # initial delay in seconds\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = session.get(url, timeout=10)\n",
    "            if response.status_code == 429:\n",
    "                print(f\"Rate limit hit. Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2  # exponential backoff\n",
    "                continue\n",
    "            elif response.status_code != 200:\n",
    "                print(f\"Failed to fetch {url} with status code {response.status_code}\")\n",
    "                return None\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            scheme_name = soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\").text.strip() if soup.find('h1', class_=\"font-bold text-xl sm:text-2xl mt-1 text-[#24262B] dark:text-white\") else 'N/A'\n",
    "            detail = soup.find('div', id='details').text.strip() if soup.find('div', id='details') else 'N/A'\n",
    "            benefit = soup.find('div', id='benefits').text.strip() if soup.find('div', id='benefits') else 'N/A'\n",
    "            eligibility = soup.find('div', id='eligibility').text.strip() if soup.find('div', id='eligibility') else 'N/A'\n",
    "            exclusion = soup.find('div', id='exclusions').text.strip() if soup.find('div', id='exclusions') else 'N/A'\n",
    "            application_process = soup.find('div', id='application-process').text.strip() if soup.find('div', id='application-process') else 'N/A'\n",
    "            documents_required = soup.find('div', id='documents-required').text.strip() if soup.find('div', id='documents-required') else 'N/A'\n",
    "            faq = soup.find('div', id='faqs').text.strip() if soup.find('div', id='faqs') else 'N/A'\n",
    "            keyword = soup.find('div', class_='mb-2 md:mb-0 w-full').text.strip() if soup.find('div', class_='mb-2 md:mb-0 w-full') else 'N/A'\n",
    "            \n",
    "            return {\n",
    "                'SchemeName': scheme_name,\n",
    "                'Details': detail,\n",
    "                'Benefits': benefit,\n",
    "                'Eligibility': eligibility,\n",
    "                'Exclusion': exclusion,\n",
    "                'ApplicationProc': application_process,\n",
    "                'DocumentsReq': documents_required,\n",
    "                'FAQ': faq,\n",
    "                'Link': url,\n",
    "                'Keywords': keyword\n",
    "            }\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request error scraping {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.myscheme.gov.in/search\"\n",
    "    print(1)\n",
    "    \n",
    "    # Setup Selenium and extract links\n",
    "    driver, wait = setup_selenium()\n",
    "    print(2)\n",
    "    all_links = scrape_all_links(driver, wait, base_url)\n",
    "\n",
    "    print(f\"Unique {len(all_links)} links.\")\n",
    "\n",
    "    # Load existing data if available\n",
    "    if os.path.exists('scheme_sample3.csv'):\n",
    "        existing_data = pd.read_csv('scheme_sample3.csv')\n",
    "    else:\n",
    "        existing_data = pd.DataFrame()\n",
    "    \n",
    "    # Initialize lists to store new data\n",
    "    new_data = []\n",
    "\n",
    "    # Create a requests session with retries\n",
    "    session = requests_session_with_retries()\n",
    "\n",
    "    # Use ThreadPoolExecutor to manage concurrent scraping\n",
    "    with ThreadPoolExecutor(max_workers=20) as executor:  # Increased workers for faster scraping\n",
    "        futures = [executor.submit(scrape_scheme_page, session, url) for url in all_links]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                new_data.append(result)\n",
    "                print(f\"Scraped data from {result['Link']}\")\n",
    "                time.sleep(random.uniform(0.5, 1.5))  # Reduced sleep to speed up processing\n",
    "\n",
    "    # Create a pandas DataFrame for new data\n",
    "    new_data_df = pd.DataFrame(new_data)\n",
    "    print(f\"Scraped data for {len(new_data_df)} schemes.\")\n",
    "\n",
    "    # Combine new data with existing data\n",
    "    if not existing_data.empty:\n",
    "        combined_data = pd.concat([existing_data, new_data_df])\n",
    "    else:\n",
    "        combined_data = new_data_df\n",
    "\n",
    "    # Remove duplicates based on 'Link' and keep the latest\n",
    "    combined_data = combined_data.drop_duplicates(keep='last')\n",
    "    \n",
    "    # Remove duplicates based on 'SchemeName' and keep the latest\n",
    "    combined_data = combined_data.sort_values('Link').drop_duplicates(subset=['SchemeName'], keep='last')\n",
    "\n",
    "    # Save the combined data to CSV file\n",
    "    combined_data.to_csv('scheme_sample3.csv', index=False)\n",
    "    print(\"Scraping and updating complete. Data saved to scheme_sample3.csv.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
